{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26b64234",
   "metadata": {},
   "source": [
    "# Model\n",
    ">* You can follow your work in HW3 to complete this file\n",
    ">* You can not modify the initialize_parameters in Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f78d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e61eeacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense():\n",
    "    def __init__(self, n_x, n_y, seed=1):\n",
    "        self.n_x = n_x\n",
    "        self.n_y = n_y\n",
    "        self.seed = seed\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Argument:\n",
    "        self.n_x -- size of the input layer\n",
    "        self.n_y -- size of the output layer\n",
    "        self.parameters -- python dictionary containing your parameters:\n",
    "                           W -- weight matrix of shape (n_x, n_y)\n",
    "                           b -- bias vector of shape (1, n_y)\n",
    "        \"\"\"\n",
    "        sd = np.sqrt(6.0 / (self.n_x + self.n_y))\n",
    "        np.random.seed(self.seed)\n",
    "        W = np.random.uniform(-sd, sd, (self.n_y, self.n_x)).T      # the transpose here is just for the code to be compatible with the old codes\n",
    "        b = np.zeros((1, self.n_y))\n",
    "\n",
    "        assert(W.shape == (self.n_x, self.n_y))\n",
    "        assert(b.shape == (1, self.n_y))\n",
    "\n",
    "        self.parameters = {\"W\": W, \"b\": b}\n",
    "\n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "        Arguments:\n",
    "        A -- activations from previous layer (or input data) with the shape (n, f^[l-1])\n",
    "        self.cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "\n",
    "        Returns:\n",
    "        Z -- the input of the activation function, also called pre-activation parameter with the shape (n, f^[l])\n",
    "        \"\"\"\n",
    "\n",
    "        ### START CODE HERE ### (≈ 2 line of code)\n",
    "        Z = None\n",
    "        self.cache = None\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        assert(Z.shape == (A.shape[0], self.parameters[\"W\"].shape[1]))\n",
    "\n",
    "        return Z\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "        Arguments:\n",
    "        dZ -- Gradient of the loss with respect to the linear output (of current layer l), same shape as Z\n",
    "        self.cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "        self.dW -- Gradient of the loss with respect to W (current layer l), same shape as W\n",
    "        self.db -- Gradient of the loss with respect to b (current layer l), same shape as b\n",
    "\n",
    "        Returns:\n",
    "        dA_prev -- Gradient of the loss with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "\n",
    "        \"\"\"\n",
    "        A_prev, W, b = self.cache\n",
    "        m = A_prev.shape[0]\n",
    "\n",
    "        ### START CODE HERE ### (≈ 3 lines of code)\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        dA_prev = None\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        assert (dA_prev.shape == A_prev.shape)\n",
    "        assert (self.dW.shape == self.parameters[\"W\"].shape)\n",
    "        assert (self.db.shape == self.parameters[\"b\"].shape)\n",
    "\n",
    "        return dA_prev\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update parameters using gradient descent\n",
    "\n",
    "        Arguments:\n",
    "        learning rate -- step size\n",
    "        \"\"\"\n",
    "\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        self.parameters[\"W\"] = None\n",
    "        self.parameters[\"b\"] = None\n",
    "        ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeac387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation():\n",
    "    def __init__(self, activation_function, loss_function, alpha=None, gamma=None):\n",
    "        self.activation_function = activation_function\n",
    "        self.loss_function = loss_function\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, Z):\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            \"\"\"\n",
    "            Implements the sigmoid activation in numpy\n",
    "\n",
    "            Arguments:\n",
    "            Z -- numpy array of any shape\n",
    "            self.cache -- stores Z as well, useful during backpropagation\n",
    "\n",
    "            Returns:\n",
    "            A -- output of sigmoid(z), same shape as Z\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "            ### START CODE HERE ### (≈ 8 lines of code)\n",
    "            A = None\n",
    "            self.cache = None\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            return A\n",
    "        elif self.activation_function == \"relu\":\n",
    "            \"\"\"\n",
    "            Implement the RELU function in numpy\n",
    "            Arguments:\n",
    "            Z -- numpy array of any shape\n",
    "            self.cache -- stores Z as well, useful during backpropagation\n",
    "            Returns:\n",
    "            A -- output of relu(z), same shape as Z\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            ### START CODE HERE ### (≈ 2 lines of code)\n",
    "            A = None\n",
    "            self.cache = None\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            assert(A.shape == Z.shape)\n",
    "\n",
    "            return A\n",
    "        elif self.activation_function == \"softmax\":\n",
    "            \"\"\"\n",
    "            Implements the softmax activation in numpy\n",
    "\n",
    "            Arguments:\n",
    "            Z -- np.array with shape (n, C)\n",
    "            self.cache -- stores Z as well, useful during backpropagation\n",
    "\n",
    "            Returns:\n",
    "            A -- output of softmax(z), same shape as Z\n",
    "            \"\"\"\n",
    "\n",
    "            ### START CODE HERE ### (≈ 3 lines of code)\n",
    "            tmp = None\n",
    "            A = None\n",
    "            self.cache = None\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            return A\n",
    "        else:\n",
    "            assert 0, f\"you're using undefined activation function {self.activation_function}\"\n",
    "\n",
    "\n",
    "    def backward(self, dA=None, Y=None):\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            \"\"\"\n",
    "            Implement the backward propagation for a single SIGMOID unit.\n",
    "            Arguments:\n",
    "            dA -- post-activation gradient, of any shape\n",
    "            self.cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "            Returns:\n",
    "            dZ -- Gradient of the loss with respect to Z\n",
    "            \"\"\"\n",
    "\n",
    "            ### START CODE HERE ### (≈ 9 lines of code)\n",
    "            Z = None\n",
    "            s = None\n",
    "            dZ = None\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            assert (dZ.shape == Z.shape)\n",
    "\n",
    "            return dZ\n",
    "\n",
    "        elif self.activation_function == \"relu\":\n",
    "            \"\"\"\n",
    "            Implement the backward propagation for a single RELU unit.\n",
    "            Arguments:\n",
    "            dA -- post-activation gradient, of any shape\n",
    "            self.cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "            Returns:\n",
    "            dZ -- Gradient of the loss with respect to Z\n",
    "            \"\"\"\n",
    "\n",
    "            ### START CODE HERE ### (≈ 3 lines of code)\n",
    "            Z = None\n",
    "            dZ = None\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            assert (dZ.shape == Z.shape)\n",
    "\n",
    "            return dZ\n",
    "\n",
    "        elif self.activation_function == \"softmax\" and self.loss_function == 'cross_entropy':\n",
    "            \"\"\"\n",
    "            Implement the backward propagation for a [SOFTMAX->CCE LOSS] unit.\n",
    "            Arguments:\n",
    "            Y -- true \"label\" vector (one hot vector, for example: [1,0,0] represents rock, [0,1,0] represents paper, [0,0,1] represents scissors\n",
    "                                      in a Rock-Paper-Scissors, shape: (n, C)\n",
    "            self.cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "            Returns:\n",
    "            dZ -- Gradient of the cost with respect to Z\n",
    "            \"\"\"\n",
    "\n",
    "            # GRADED FUNCTION: softmax_CCE_backward\n",
    "            ### START CODE HERE ### (≈ 3 lines of code)\n",
    "            Z = None\n",
    "            s = None\n",
    "            dZ = None\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            assert (dZ.shape == self.cache.shape)\n",
    "\n",
    "            return dZ\n",
    "        elif self.activation_function == \"softmax\" and self.loss_function == 'focal_loss':\n",
    "            \"\"\"\n",
    "            Implement the backward propagation for a [SOFTMAX->FOCAL LOSS] unit.\n",
    "            Arguments:\n",
    "            Y -- true \"label\" vector (one hot vector, for example: [1,0,0] represents rock, [0,1,0] represents paper, [0,0,1] represents scissors\n",
    "                                      in a Rock-Paper-Scissors, shape: (n, C)\n",
    "            self.cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "            Returns:\n",
    "            dZ -- Gradient of the cost with respect to Z\n",
    "            alpha -- weighting factors correspond to each class, shape: (C,)\n",
    "            gamma -- modulating factor, a float\n",
    "            \"\"\"\n",
    "\n",
    "            # FUNCTION: softmax_focalLoss_backward\n",
    "            ## START CODE HERE ### (≈ 10 lines of code)\n",
    "            None\n",
    "            ## END CODE HERE ###\n",
    "\n",
    "            assert (dZ.shape == self.cache.shape)\n",
    "\n",
    "            return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f38e1807",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.units = config.layers_dims\n",
    "        self.activation_functions = config.activation_fn\n",
    "        self.loss_function = config.loss_function\n",
    "        self.alpha = config.alpha\n",
    "        self.gamma = config.gamma\n",
    "        self.initialize_parameters()\n",
    "        self.check = True\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        self.units -- number of nodes/units for each layer, starting from the input dimension and ending with the output dimension (i.e., [4, 4, 1])\n",
    "        self.activation_functions -- activation functions used in each layer (i.e, [\"relu\", \"sigmoid\"])\n",
    "        self.loss_function -- [\"cross_entropy\", \"focal_loss\"]\n",
    "        self.alpha -- weighting factors used by focal loss correspond to each class, shape: (C,)\n",
    "        self.gamma -- a float, used by focal loss\n",
    "        \"\"\"\n",
    "        self.linear = []        # a list to store the dense layers when initializing the model\n",
    "        self.activation = []    # a list to store the activation function layers when initializing the model\n",
    "\n",
    "        \n",
    "        # FUNCTION: model_initialize_parameters\n",
    "        ### DO NOT MODIFY THIS PART ###\n",
    "        for i in range(len(self.units)-1):\n",
    "            dense = Dense(self.units[i], self.units[i+1], i)\n",
    "            self.linear.append(dense)\n",
    "\n",
    "        for i in range(len(self.activation_functions)):\n",
    "            self.activation.append(Activation(self.activation_functions[i], self.loss_function, self.alpha, self.gamma))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        X -- input data: shape (n, f)\n",
    "\n",
    "        Returns:\n",
    "        A -- output of L-layer neural network, probability vector corresponding to your label predictions, shape (n, C)\n",
    "        \"\"\"\n",
    "        A = X\n",
    "\n",
    "        # GRADED FUNCTION: model_forward\n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        None\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return A\n",
    "\n",
    "    def backward(self, AL=None, Y=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        For multi-class classification,\n",
    "        AL -- output of L-layer neural network, probability vector corresponding to your label predictions, shape (n, C)\n",
    "        Y -- true \"label\" vector (one hot vector, for example: [1,0,0] represents rock, [0,1,0] represents paper, [0,0,1] represents scissors\n",
    "                                      in a Rock-Paper-Scissors, shape: (n, C)\n",
    "\n",
    "        Returns:\n",
    "        dA_prev -- post-activation gradient\n",
    "        \"\"\"\n",
    "\n",
    "        L = len(self.linear)\n",
    "        C = Y.shape[1]\n",
    "\n",
    "        # assertions\n",
    "        warning = 'Warning: only the following 4 combinations are allowed! \\n \\\n",
    "                    1. binary classification: sigmoid + cross_entropy) \\n \\\n",
    "                    2. binary classification: softmax + focal_loss) \\n \\\n",
    "                    3. multi-class classification: softmax + cross_entropy) \\n \\\n",
    "                    4. multi-class classification: softmax + focal_loss)'\n",
    "        assert self.loss_function in [\"cross_entropy\", \"focal_loss\"], \"you're using undefined loss function!\"\n",
    "        if Y.shape[1] <= 2:                                 # in binary classification\n",
    "            if self.loss_function == \"cross_entropy\":\n",
    "                assert self.activation_functions[-1] == 'sigmoid', warning\n",
    "                assert self.units[-1] == 1, \"you should set last dim to 1 when using sigmoid + cross_entropy in binary classification!\"\n",
    "            elif self.loss_function  == \"focal_loss\":\n",
    "                assert self.activation_functions[-1] == 'softmax', warning\n",
    "                assert self.units[-1] == 2, \"you should set last dim to 2 when using softmax + focal_loss in binary classification!\"\n",
    "        else:                                               # in multi-class classification\n",
    "            assert self.activation_functions[-1] == 'softmax', warning\n",
    "            assert self.units[-1] == Y.shape[1], f\"you should set last dim to {Y.shape[1]}(the number of classes) in multi-class classification!\"\n",
    "\n",
    "        # FUNCTION: model_backward\n",
    "        ### START CODE HERE ### (≈ 20 lines of code)\n",
    "\n",
    "        if self.activation_functions[-1] == \"sigmoid\":\n",
    "            if self.loss_function == 'cross_entropy':\n",
    "                # Initializing the backpropagation\n",
    "                dAL = None\n",
    "\n",
    "                # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL\". Outputs: \"dA_prev\"\n",
    "                dZ = None\n",
    "                dA_prev = None\n",
    "        elif self.activation_functions[-1] == \"softmax\":\n",
    "            # Initializing the backpropagation\n",
    "            dZ = None\n",
    "\n",
    "            # Lth layer (LINEAR) gradients. Inputs: \"dZ\". Outputs: \"dA_prev\"\n",
    "            dA_prev = None\n",
    "\n",
    "        # Loop from l=L-2 to l=0\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"dA_prev\". Outputs: \"dA_prev\"\n",
    "        None\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return dA_prev\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        learning_rate -- step size\n",
    "        \"\"\"\n",
    "\n",
    "        L = len(self.linear)\n",
    "\n",
    "        # FUNCTION: model_update_parameters\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        None\n",
    "        ### END CODE HERE ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

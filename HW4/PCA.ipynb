{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c97bfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Data_preprocess.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import import_ipynb\n",
    "from Data_preprocess import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb17ac2",
   "metadata": {},
   "source": [
    "### Steps of PCA:\n",
    "\n",
    "1. **Centeralize the Data:**\n",
    "   - Centeralize the dataset by subtracting the mean\n",
    "   $$X_{\\text{cent}} = X - \\bar{X}$$\n",
    "\n",
    "2. **Calculate the Covariance Matrix:**\n",
    "   - Calculate the covariance matrix, which represents the relationships between different features.\n",
    "   $$ \\text{Cov}(X, Y) = \\sum_{i=1}^{n}(x_i - \\bar{X})(y_i - \\bar{Y})\\ $$\n",
    "\n",
    "3. **Compute Eigenvectors and Eigenvalues:**\n",
    "   - Calculate the eigenvectors and eigenvalues of the covariance matrix.\n",
    "   $$ \\text{Covariance Matrix} \\times \\text{Eigenvector} = \\text{Eigenvalue} \\times \\text{Eigenvector} $$\n",
    "\n",
    "4. **Sort Eigenvectors by Eigenvalues:**\n",
    "   - Sort the eigenvectors in descending order based on their corresponding eigenvalues.\n",
    "\n",
    "5. **Select Principal Components:**\n",
    "   - Choose the top $k$ eigenvectors (principal components) corresponding to the $k$ largest eigenvalues to form the projection matrix $W$.\n",
    "   $$ W = \\begin{bmatrix} \\text{eigenvector}_1 & \\text{eigenvector}_2 & \\ldots & \\text{eigenvector}_k \\end{bmatrix} $$\n",
    "\n",
    "6. **Transform the Data:**\n",
    "   - Project the original data onto the new feature subspace using the projection matrix $W$.\n",
    "   $$ \\text{Transformed Data} = \\text{Original Data} \\times W $$\n",
    "\n",
    "### Formula Notes:\n",
    "- $n$ is the number of data points.\n",
    "- $X$ and $Y$ are two variables.\n",
    "- $\\bar{X}$ and $\\bar{Y}$ are the means of $X$ and $Y$, respectively.\n",
    "- $\\text{Eigenvector}$ and $\\text{Eigenvalue}$ are obtained from the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e42be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MY_PCA():\n",
    "    '''\n",
    "    Please the PCA function here\n",
    "    \n",
    "    fit_transform -- Fit the model with input data and apply the dimensionality reduction on it.\n",
    "    transform -- Apply dimensionality reduction to input data\n",
    "    components_remain_ratio -- Calculate the minimum number of components needed to retain a specific proportion of the original data's information.\n",
    "    reconstructData -- Reconstruct the data from top k eigenvectors\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, n_components=2):\n",
    "        self.n_components = n_components\n",
    "        self.data = None\n",
    "        self.eigen_vec = None\n",
    "        self.eigen_val = None\n",
    "        self.principal_components = None\n",
    "        self.covariance_matrix = None\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def PCA_visualization(data_pca, label, text=False, n_components=2, tag=None):\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        if len(label.shape) > 1:\n",
    "            label = np.argmax(label, axis=1)\n",
    "#         Plot the data in the reduced dimensional space\n",
    "        if text:\n",
    "            x_min, x_max = np.min(data_pca, 0), np.max(data_pca, 0)\n",
    "            data_pca = (data_pca - x_min) / (x_max - x_min)\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            for i in range(data_pca.shape[0]):\n",
    "                if n_components == 2:\n",
    "                    plt.text(data_pca[i, 0], data_pca[i, 1], str(label[i]), \n",
    "                             color=plt.cm.Set1(label[i]), \n",
    "                             fontdict={'size': 15})\n",
    "                elif n_components == 1:\n",
    "                    plt.text(data_pca[i], np.zeros(data_pca.shape[0]), str(label[i]), \n",
    "                             color=plt.cm.Set1(label[i]), \n",
    "                             fontdict={'size': 15})\n",
    "                    \n",
    "            plt.xticks([]), plt.yticks([]), plt.ylim([-0.1,1.1]), plt.xlim([-0.1,1.1])\n",
    "\n",
    "        else:\n",
    "            if n_components == 2:\n",
    "                plt.scatter(data_pca[:, 0], data_pca[:, 1], c=label, cmap='viridis', alpha=0.7)\n",
    "            elif n_components == 1:\n",
    "                plt.scatter(data_pca, np.zeros(data_pca.shape[0]), c=label, cmap='viridis', alpha=0.7)\n",
    "                \n",
    "            plt.colorbar(label='Label')\n",
    "            plt.tight_layout()\n",
    "\n",
    "        plt.title(f'{tag} Data in Reduced Dimensional Space (PCA) with Colored Labels')\n",
    "        plt.xlabel('Principal Component 1')\n",
    "        plt.ylabel('Principal Component 2')\n",
    "        plt.savefig(f'PCA_{tag}.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        return\n",
    "\n",
    "        \n",
    "    def _COVARIANCE_MATRIX_COMPUTATION(self, centerized_data):\n",
    "        # GRADED CODE: PCA COVARIANCE MATRIX COMPUTATION\n",
    "        ### !!! In this part, you can only use the common matrix operation in numpy. !!! ###\n",
    "        ### !!! Please Do Not use another functions or libraries in this part. !!! ###\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        self.covariance_matrix = None\n",
    "        ### END CODE HERE ###\n",
    "        return self.covariance_matrix\n",
    "    \n",
    "        \n",
    "    def _COMPUTE_THE_EIGENVECTORS_AND_EIGENVALUES(self, cov_mat):\n",
    "        # GRADED CODE: PCA EIGENVECTORS AND EIGENVALUES COMPUTATION\n",
    "        ### !!! In this part, you can use any numpy functions. !!! ###\n",
    "        ### !!! If you are using the numpy library, it is recommended to use np.linalg.eigh(). !!!### \n",
    "    \n",
    "        ### START CODE HERE ###\n",
    "        eigen_val, eigen_vec = None\n",
    "        sorted_indices = None\n",
    "        self.eigen_val = None\n",
    "        self.eigen_vec = None\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def components_remain_ratio(self, ratio):\n",
    "        # GRADED CODE: RECONSTRUCT DATA\n",
    "        ### START CODE HERE ###\n",
    "        total_var = None\n",
    "        var_ratio = None\n",
    "        cumulative_var = None\n",
    "        num_components = None\n",
    "        return num_components, var_ratio\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    \n",
    "    def reconstructData(self, data_x, mean_data, k):\n",
    "        '''\n",
    "        reconstruct_data -- the data reconstructed by top k eigenvector\n",
    "        z -- the list of coefficients for top k eigenvector\n",
    "        '''\n",
    "        # GRADED CODE: RECONSTRUCT DATA\n",
    "        ### START CODE HERE ###\n",
    "        None\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        return reconstruct_data, z\n",
    "    \n",
    "    def transform(self, data_X):\n",
    "        # GRADED CODE: PCA TRANSFORM FUNCTION\n",
    "        ### START CODE HERE ###\n",
    "        data_pca = None\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        return data_pca\n",
    "        \n",
    "    def fit_transform(self, data_X):\n",
    "        \n",
    "        # GRADED CODE: PCA FITTING FUNCTION\n",
    "        ### START CODE HERE ###\n",
    "        self.data = None\n",
    "        covariance_matrix = None\n",
    "        self._COMPUTE_THE_EIGENVECTORS_AND_EIGENVALUES(covariance_matrix)\n",
    "        self.principal_components = None\n",
    "        data_pca = None\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        return data_pca\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d92ce68",
   "metadata": {},
   "source": [
    "### Steps of Sparse PCA:\n",
    "\n",
    "1. **Centeralize the Data:**\n",
    "   - Centeralize the dataset by subtracting the mean\n",
    "   $$X_{\\text{cent}} = X - \\bar{X}$$\n",
    "\n",
    "2. **Initialize Components:**\n",
    "   - - Initialize the loading matrix $V$ using the transposed loading matrix from SVD.\n",
    "   $$ V \\text{ (Initialize)} $$\n",
    "\n",
    "3. **Iterative Thresholding:**\n",
    "   - Perform iterative thresholding to enforce sparsity on the loading matrix $V$.\n",
    "      $$ V_{\\text{new}} = \\text{soft\\_threshold}(V_{\\text{old}}, \\alpha) $$\n",
    "      where $$ \\text{soft\\_threshold}(x, \\alpha) = \\text{sign}(x) \\cdot \\max(|x| - \\alpha, 0) $$\n",
    "\n",
    "4. **Normalize Components:**\n",
    "   - Normalize the loading matrix $V$ to maintain unit length.\n",
    "     $$ V = \\frac{V}{\\|V\\|_2} $$\n",
    "\n",
    "5. **Repeat Iterations:**\n",
    "   - Repeat steps 3-4 for a specified number of iterations or until convergence based on the change in sum of squared differences (SSD).\n",
    "\n",
    "6. **Transform the Data:**\n",
    "   - Project the original data onto the sparse principal components $V$.\n",
    "      $$ \\text{Transformed Data} = \\text{Original Data} \\cdot V $$\n",
    "\n",
    "### Formula Notes:\n",
    "- $X$ is the original data matrix.\n",
    "- $X_{\\text{cent}}$ is the centerized data matrix.\n",
    "- $V$ is the matrix of sparse principal components.\n",
    "- $\\alpha$ is the sparsity-inducing parameter.\n",
    "- $\\text{soft\\_threshold}(x, \\alpha)$ is the soft thresholding function.\n",
    "- $\\text{sign}(x)$ returns the sign of $x$.\n",
    "- $\\|\\cdot\\|_2$ denotes the L2 norm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac49e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MY_SparsePCA():\n",
    "    def __init__(self, n_components, alpha, max_iter=1000, tol=1e-6):\n",
    "        self.alpha = alpha\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.components_ = None\n",
    "        self.error_ = None\n",
    "        self.S = None\n",
    "        self.Vt = None\n",
    "        \n",
    "    @staticmethod\n",
    "    def soft_threshold(x, alpha):\n",
    "        # GRADED CODE: Iterative Thresholding\n",
    "        ### START CODE HERE ###\n",
    "        # Soft threshold function used for SPCA.\n",
    "        rt = None\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        return rt\n",
    "\n",
    "    def fit_transform(self, x):\n",
    "        \n",
    "        # GRADED CODE: SPARSE PCA FITTINT FUNCTION\n",
    "        ### START CODE HERE ###\n",
    "        # Initialize Components:\n",
    "        _, S, Vt = np.linalg.svd(x, full_matrices=False)\n",
    "        self.components_ = None\n",
    "        self.S = S\n",
    "        self.Vt = Vt\n",
    "        \n",
    "        for iteration in tqdm(range(self.max_iter)):\n",
    "            \n",
    "            # Compute the transform in this iteration\n",
    "            x_projected = None\n",
    "            \n",
    "            # Iterative Thresholding:\n",
    "            self.components_ = None\n",
    "            \n",
    "            # Normalize Components:\n",
    "            norm = None\n",
    "            norm[norm < 1e-10] = 1.0\n",
    "            self.components_ /= None\n",
    "            \n",
    "            # SSD Check\n",
    "            ssd = None\n",
    "            if ssd < self.tol:\n",
    "                break\n",
    "        # Transform the data\n",
    "        x_transformed = None\n",
    "        \n",
    "        ### END CODE HERE###\n",
    "        \n",
    "        return x_transformed\n",
    "\n",
    "    def fit(self, x):\n",
    "        self.fit_transform(x)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        return x@self.components_\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('biopsy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc07a8c6aa785ccbb5cb0815abffaffb139b111aca417c4ffe9bf221bae76ba2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{"cells":[{"cell_type":"markdown","metadata":{"id":"IagZMs0_qjdL"},"source":["# 1. Introduction\n","\n","Welcome to your third assignment. In this assignment, you will build a deep neural network step by step. In this notebook, you will implement all the functions required to build a neural network.\n","\n","After finishing this assignment, you will have a deeper understanding of the process of training a deep neural network, which only consists of three steps: forward propagation, backward propagation and update."]},{"cell_type":"markdown","metadata":{"id":"yGFR00CQvoaH"},"source":["# 2. Important notice\n","\n","## 2.1 Packages\n","All the packages that you need to finish this assignment are listed below.\n","*   numpy : the fundamental package for scientific computing with Python.\n","*   matplotlib : a comprehensive library for creating static, animated, and interactive visualizations in Python.\n","*   math : Python has a built-in module that you can use for mathematical tasks.\n","*   sklearn.metrics: we use this to compute the f1 score\n","*   from google.colab import drive: used to access data in your google drive\n","\n","⚠️ **WARNING** ⚠️:\n","*   Please do not import any other packages.\n","*   np.random.seed(1) is used to keep all the random function calls consistent. It will help us grade your work. Please don't change the seed.\n","\n","## 2.2 Todo\n","```\n","### START CODE HERE ### (≈ n lines of code)\n","...\n","### END CODE HERE ###\n","```\n","❗ **Important** ❗: Please do not change the code outside this code bracket in the first part.\n","\n","### Common Notation\n","* $C$: number of classes\n","* $n$: number of samples\n","* $f^{[l]}$: the dimension of outputs in layer $l$, but $f^{[0]}$ is the input dimension\n","* $Z^{[l]} = A^{[l-1]}W^{[l]} + b^{[l]}$\n","    * $Z^{[l]}$: the output of layer $l$ in the shape $(n, f^{[l]})$\n","    * $A^{[l]}$: the activation of $Z^{[l]}$ in the shape $(n, f^{[l]})$, but $A^{[0]}$ is input $X$\n","    * $W^{[l]}$: the weight in layer $l$ in the shape $(f^{[l-1]}, f^{[l]})$\n","    * $b^{[l]}$: the bias in layer $l$ in the shape $(1, f^{[l]})$\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"fmTH9UkeqdYf"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","from sklearn.metrics import f1_score\n","\n","outputs = {}"]},{"cell_type":"markdown","metadata":{"id":"w35ZkTwMc00G"},"source":["# 3. Neural network\n","In this section, you will need to implement a deep neural network from scratch all by yourself. If you are familiar with deep learning library, such as Tensorflow or PyTorch, it may seems easy for you. But if you don't, don't worry because we will guide you step by step. All you need to do is to follow the instructions and understand how each part works.\n","\n","As mentioned before, the process of training a deep neural network is composed of three steps: forward propagation, backward propagation, and update, so all the to-do in this section will be related to these three steps."]},{"cell_type":"markdown","metadata":{"id":"P_krGKUNg_Ix"},"source":["## 3.1 Implement a linear layer (10%)\n","First, we will start by implementing one of the most commonly used layers in the deep neural network, called the dense layer. The dense layer is a linear layer applying a linear transformation to the incoming data:\n","$Z = AW + b$, where $W$ and $b$ are the weight and bias.\n","\n","**Note**: Dense layers, also known as Fully-connected layers, connect every input neuron to every output neuron and are commonly used in neural networks.\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"x0KHo8w9yqbY"},"outputs":[],"source":["class Dense():\n","    def __init__(self, n_x, n_y, seed=1):\n","        self.n_x = n_x\n","        self.n_y = n_y\n","        self.seed = seed\n","        self.initialize_parameters()\n","\n","    def initialize_parameters(self):\n","        \"\"\"\n","        Argument:\n","        self.n_x -- size of the input layer\n","        self.n_y -- size of the output layer\n","        self.parameters -- python dictionary containing your parameters:\n","                           W -- weight matrix of shape (n_x, n_y)\n","                           b -- bias vector of shape (1, n_y)\n","        \"\"\"\n","        sd = np.sqrt(6.0 / (self.n_x + self.n_y))\n","        np.random.seed(self.seed)\n","        W = np.random.uniform(-sd, sd, (self.n_y, self.n_x)).T      # the transpose here is just for the code to be compatible with the old codes\n","        b = np.zeros((1, self.n_y))\n","\n","        assert(W.shape == (self.n_x, self.n_y))\n","        assert(b.shape == (1, self.n_y))\n","\n","        self.parameters = {\"W\": W, \"b\": b}\n","\n","    def forward(self, A):\n","        \"\"\"\n","        Implement the linear part of a layer's forward propagation.\n","\n","        Arguments:\n","        A -- activations from previous layer (or input data) with the shape (n, f^[l-1])\n","        self.cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n","\n","        Returns:\n","        Z -- the input of the activation function, also called pre-activation parameter with the shape (n, f^[l])\n","        \"\"\"\n","\n","        # GRADED FUNCTION: linear_forward\n","        ### START CODE HERE ### (≈ 2 line of code)\n","        Z = A @ self.parameters['W'] + self.parameters['b']\n","        self.cache = (A, self.parameters['W'], self.parameters['b'])\n","        ### END CODE HERE ###\n","\n","        assert(Z.shape == (A.shape[0], self.parameters[\"W\"].shape[1]))\n","\n","        return Z\n","\n","    def backward(self, dZ):\n","        \"\"\"\n","        Implement the linear portion of backward propagation for a single layer (layer l)\n","\n","        Arguments:\n","        dZ -- Gradient of the loss with respect to the linear output (of current layer l), same shape as Z\n","        self.cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n","        self.dW -- Gradient of the loss with respect to W (current layer l), same shape as W\n","        self.db -- Gradient of the loss with respect to b (current layer l), same shape as b\n","\n","        Returns:\n","        dA_prev -- Gradient of the loss with respect to the activation (of the previous layer l-1), same shape as A_prev\n","\n","        \"\"\"\n","        A_prev, W, b = self.cache\n","        m = A_prev.shape[0]\n","\n","        # GRADED FUNCTION: linear_backward\n","        ### START CODE HERE ### (≈ 3 lines of code)\n","        self.dW = (1/m) * A_prev.T @ dZ\n","        self.db = (1/m) * dZ.sum(axis=0, keepdims=True)\n","        dA_prev = dZ @ W.T\n","        ### END CODE HERE ###\n","\n","        assert (dA_prev.shape == A_prev.shape)\n","        assert (self.dW.shape == self.parameters[\"W\"].shape)\n","        assert (self.db.shape == self.parameters[\"b\"].shape)\n","\n","        return dA_prev\n","\n","    def update(self, learning_rate):\n","        \"\"\"\n","        Update parameters using gradient descent\n","\n","        Arguments:\n","        learning rate -- step size\n","        \"\"\"\n","\n","        # GRADED FUNCTION: linear_update_parameters\n","        ### START CODE HERE ### (≈ 2 lines of code)\n","        self.parameters[\"W\"] -= learning_rate * self.dW\n","        self.parameters[\"b\"] -= learning_rate * self.db\n","        ### END CODE HERE ###"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"y442Z9Ivvidd"},"source":["\n","### 3.1.1. Initialize parameters (0%)\n","*   You don't need to write this part.\n","*   We use random initialization (uniform distribution) for the weight matrices. Draws samples from a uniform distribution within $[\\text{-limit}, \\text{limit}]$, where $\\text{limit} = \\sqrt{\\frac{6}{\\text{fan\\_in} + \\text{fan\\_out}}}$ (fan_in is the number of input units in the weight tensor and fan_out is the number of output units).\n","*   Use zero initialization for the biases."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"7HNAWwmg8R7T"},"outputs":[{"name":"stdout","output_type":"stream","text":["W = [[-0.20325375]\n"," [ 0.53968259]\n"," [-1.22446471]]\n","b = [[0.]]\n"]}],"source":["dense = Dense(3, 1)\n","print(\"W = \" + str(dense.parameters[\"W\"]))\n","print(\"b = \" + str(dense.parameters[\"b\"]))"]},{"cell_type":"markdown","metadata":{"id":"OtPtH0j3BFN7"},"source":["Expected output:\n","<table>\n","  <tr>\n","    <td>W: </td>\n","    <td>[[-0.20325375]  [0.53968259 [-1.22446471]]</td>\n","  </tr>\n","  <tr>\n","    <td>b: </td>\n","    <td>[[0.]]</td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"abu7YqxeAeMz"},"source":["### 3.1.2. Linear forward (4%)\n","\n","After initializing parameters, you will need to apply the linear transformation to the incoming data, and this can be simply done by matrix multiplication and addition.\n","\n","**Exercise**: Implement linear forward by applying the linear transformation. (5%)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"SSf8JIyjaj_A"},"outputs":[{"name":"stdout","output_type":"stream","text":["Z = [[1.9]\n"," [2.2]\n"," [2.5]]\n"]}],"source":["A, W, b = np.array([[0., 1., 2.], [0.5, 1.5, 2.5], [1., 2., 3.]]), np.array([[0.1], [0.2], [0.3]]), np.array([[1.1]])\n","dense = Dense(3, 1)\n","dense.parameters = {\"W\": W, \"b\": b}\n","Z = dense.forward(A)\n","print(\"Z = \" + str(Z))\n","\n","A, W, b = np.array([[-0.80,-0.45,-1.11],[-1.65,-2.36,1.14],[-1.02,0.64,-0.86]]), np.array([[0.3], [0.3], [0.1]]), np.array([[-6.2]])\n","dense = Dense(3, 1)\n","dense.parameters = {\"W\": W, \"b\": b}\n","Z = dense.forward(A)\n","outputs[\"linear_forward\"] = (Z, dense.cache)"]},{"cell_type":"markdown","metadata":{"id":"NpcPlE8-EUsR"},"source":["Expected output:\n","<table>\n","  <tr>\n","    <td>Z: </td>\n","    <td>[[1.9] [2.2] [2.5]]</td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"-K8_obj6vIeT"},"source":["### 3.1.3. Linear backward (4%)\n","Backpropagation is used to calculate the gradient of the loss function with respect to the parameters.\n","\n","For layer $l$, the linear part is: $Z^{[l]} = A^{[l-1]} W^{[l]} + b^{[l]}$ (followed by an activation).\n","\n","Suppose you have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. You want to get $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$.\n","\n","The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ are computed using the input $dZ^{[l]}$.\n","\n","Here are the formulas you need:\n","$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} A^{[l-1] T} dZ^{[l]}$$\n","$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)} $$\n","$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = dZ^{[l]} W^{[l] T}$$\n","\n","**Exercise**: Use the 3 formulas above to implement `linear_backward()`."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"fg-PfP31NKH7"},"outputs":[{"name":"stdout","output_type":"stream","text":["dA_prev = [[3.5]\n"," [6. ]]\n","dW = [[1.625 0.625]]\n","db = [[2.   0.75]]\n"]}],"source":["dZ, linear_cache = np.array([[1.5, 0.5], [2.5, 1.]]), (np.array([[0.5], [1]]), np.array([[2., 1.0]]), np.array([[0.5, 1.]]))\n","dense = Dense(1, 2)\n","dense.cache = linear_cache\n","dA_prev = dense.backward(dZ)\n","print (\"dA_prev = \" + str(dA_prev))\n","print (\"dW = \" + str(dense.dW))\n","print (\"db = \" + str(dense.db))\n","\n","dZ, linear_cache = np.array([[0.52,0.34],[0.76,0.89]]), (np.array([[0.42], [0.68]]), np.array([[0.35, 0.89]]), np.array([[0.12, 0.76]]))\n","dense = Dense(1, 2)\n","dense.cache = linear_cache\n","dA_prev = dense.backward(dZ)\n","outputs[\"linear_backward\"] = (dA_prev, dense.dW, dense.db)"]},{"cell_type":"markdown","metadata":{"id":"ny0k-zxuNKIB"},"source":["Expected output:\n","<table>\n","  <tr>\n","    <td>dA_prev: </td>\n","    <td>[[3.5] [6.0]]</td>\n","  </tr>\n","  <tr>\n","    <td>dW: </td>\n","    <td>[[1.625 0.625]]</td>\n","  </tr>\n","  <tr>\n","    <td>db: </td>\n","    <td>[[2.0 0.75]]</td>\n","  </tr>\n","</table>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"XWNWxxutN47B"},"source":["### 3.1.4. Linear update parameters (2%)\n","In this section you will update the parameters of the linear layer, using gradient descent:\n","\n","$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} $$\n","$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} $$\n","\n","**Exercise**: Implement update() to update your parameters using gradient descent.\n","\n","**Instructions**:\n","*   Update parameters using gradient descent on $W^{[l]}$ and $b^{[l]}$.\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"BMBqHniLN47I"},"outputs":[{"name":"stdout","output_type":"stream","text":["W = [[0.5 2.5]]\n","b = [[-1.  2.]]\n"]}],"source":["np.random.seed(1)\n","dense = Dense(1, 2)\n","dense.parameters = {\"W\": np.array([[1.0, 2.0]]), \"b\": np.array([[0.5, 0.5]])}\n","dense.dW = np.array([[0.5, -0.5]])\n","dense.db = np.array([[1.5, -1.5]])\n","dense.update(1.0)\n","print(\"W = \" + str(dense.parameters[\"W\"]))\n","print(\"b = \" + str(dense.parameters[\"b\"]))\n","\n","np.random.seed(1)\n","dense = Dense(3, 4)\n","parameters, grads = {\"W1\": np.random.rand(3, 4), \"b1\": np.random.rand(1,4)}, {\"dW1\": np.random.rand(3, 4), \"db1\": np.random.rand(1,4)}\n","dense.parameters = {\"W\": parameters[\"W1\"], \"b\": parameters[\"b1\"]}\n","dense.dW = grads[\"dW1\"]\n","dense.db = grads[\"db1\"]\n","dense.update(0.1)\n","outputs[\"linear_update_parameters\"] = {\"W\": dense.parameters[\"W\"], \"b\": dense.parameters[\"b\"]}"]},{"cell_type":"markdown","metadata":{"id":"LIl13uvgN47I"},"source":["Expected output:\n","<table>\n","  <tr>\n","    <td>W1: </td>\n","    <td>[[0.5 2.5]]</td>\n","  </tr>\n","  <tr>\n","    <td>b1: </td>\n","    <td>[[-1.  2.]]</td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"syt1bV3bdI_f"},"source":["## 3.2. Activation function layer (25%)\n","\n","In this section, you will need to implement activation function layers. There are many activation functions, such as sigmoid function, softmax function, ReLU function and etc.\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Nnuv8MmebMgg"},"outputs":[],"source":["class Activation():\n","    def __init__(self, activation_function, loss_function, alpha=None, gamma=None):\n","        self.activation_function = activation_function\n","        self.loss_function = loss_function\n","        self.alpha = alpha\n","        self.gamma = gamma\n","\n","    def forward(self, Z):\n","        if self.activation_function == \"sigmoid\":\n","            \"\"\"\n","            Implements the sigmoid activation in numpy\n","\n","            Arguments:\n","            Z -- numpy array of any shape\n","            self.cache -- stores Z as well, useful during backpropagation\n","\n","            Returns:\n","            A -- output of sigmoid(z), same shape as Z\n","            \"\"\"\n","\n","            # GRADED FUNCTION: sigmoid_forward\n","            ### START CODE HERE ### (≈ 8 lines of code)\n","            A = (np.greater(Z, 0).astype(float) + np.less_equal(Z, 0).astype(float) * np.exp(Z)) / (1 + np.exp(-np.abs(Z)))\n","            self.cache = Z\n","            ### END CODE HERE ###\n","\n","            return A\n","        elif self.activation_function == \"relu\":\n","            \"\"\"\n","            Implement the RELU function in numpy\n","            Arguments:\n","            Z -- numpy array of any shape\n","            self.cache -- stores Z as well, useful during backpropagation\n","            Returns:\n","            A -- output of relu(z), same shape as Z\n","\n","            \"\"\"\n","\n","            # GRADED FUNCTION: relu_forward\n","            ### START CODE HERE ### (≈ 2 lines of code)\n","            A = np.maximum(Z, 0)\n","            self.cache = Z\n","            ### END CODE HERE ###\n","\n","            assert(A.shape == Z.shape)\n","\n","            return A\n","        elif self.activation_function == \"softmax\":\n","            \"\"\"\n","            Implements the softmax activation in numpy\n","\n","            Arguments:\n","            Z -- np.array with shape (n, C)\n","            self.cache -- stores Z as well, useful during backpropagation\n","\n","            Returns:\n","            A -- output of softmax(z), same shape as Z\n","            \"\"\"\n","\n","            # GRADED FUNCTION: softmax_forward\n","            ### START CODE HERE ### (≈ 3 lines of code)\n","            b = np.max(Z, axis=1).reshape(-1, 1)\n","            A = np.exp(Z - b) / np.exp(Z - b).sum(axis=1).reshape(-1, 1)\n","            self.cache = Z\n","            ### END CODE HERE ###\n","\n","            return A\n","        else:\n","            assert 0, f\"you're using undefined activation function {self.activation_function}\"\n","\n","\n","    def backward(self, dA=None, Y=None):\n","        if self.activation_function == \"sigmoid\":\n","            \"\"\"\n","            Implement the backward propagation for a single SIGMOID unit.\n","            Arguments:\n","            dA -- post-activation gradient, of any shape\n","            self.cache -- 'Z' where we store for computing backward propagation efficiently\n","            Returns:\n","            dZ -- Gradient of the loss with respect to Z\n","            \"\"\"\n","\n","            # GRADED FUNCTION: sigmoid_backward\n","            ### START CODE HERE ### (≈ 9 lines of code)\n","            sigmoid = lambda x: (np.greater(x, 0).astype(float) + np.less_equal(x, 0).astype(float) * np.exp(x)) / (1 + np.exp(-np.abs(x)))\n","            Z = self.cache\n","            dZ = dA * sigmoid(Z) * (1 - sigmoid(Z))\n","            ### END CODE HERE ###\n","\n","            assert (dZ.shape == Z.shape)\n","\n","            return dZ\n","\n","        elif self.activation_function == \"relu\":\n","            \"\"\"\n","            Implement the backward propagation for a single RELU unit.\n","            Arguments:\n","            dA -- post-activation gradient, of any shape\n","            self.cache -- 'Z' where we store for computing backward propagation efficiently\n","            Returns:\n","            dZ -- Gradient of the loss with respect to Z\n","            \"\"\"\n","\n","            # GRADED FUNCTION: relu_backward\n","            ### START CODE HERE ### (≈ 3 lines of code)\n","            Z = self.cache\n","            dZ = dA * np.greater(Z, 0).astype(float)\n","            ### END CODE HERE ###\n","\n","            assert (dZ.shape == Z.shape)\n","\n","            return dZ\n","\n","        elif self.activation_function == \"softmax\" and self.loss_function == 'cross_entropy':\n","            \"\"\"\n","            Implement the backward propagation for a [SOFTMAX->CCE LOSS] unit.\n","            Arguments:\n","            Y -- true \"label\" vector (one hot vector, for example: [1,0,0] represents rock, [0,1,0] represents paper, [0,0,1] represents scissors\n","                                      in a Rock-Paper-Scissors, shape: (n, C)\n","            self.cache -- 'Z' where we store for computing backward propagation efficiently\n","            Returns:\n","            dZ -- Gradient of the cost with respect to Z\n","            \"\"\"\n","\n","            # GRADED FUNCTION: softmax_CCE_backward\n","            ### START CODE HERE ### (≈ 3 lines of code)\n","            Z = self.cache\n","            s = np.exp(Z - (b := np.max(Z, axis=1).reshape(-1, 1))) / np.exp(Z - b).sum(axis=1).reshape(-1, 1)\n","            dZ = s - Y\n","            ### END CODE HERE ###\n","\n","            assert (dZ.shape == self.cache.shape)\n","\n","            return dZ\n","        elif self.activation_function == \"softmax\" and self.loss_function == 'focal_loss':\n","            \"\"\"\n","            Implement the backward propagation for a [SOFTMAX->FOCAL LOSS] unit.\n","            Arguments:\n","            Y -- true \"label\" vector (one hot vector, for example: [1,0,0] represents rock, [0,1,0] represents paper, [0,0,1] represents scissors\n","                                      in a Rock-Paper-Scissors, shape: (n, C)\n","            self.cache -- 'Z' where we store for computing backward propagation efficiently\n","            Returns:\n","            dZ -- Gradient of the cost with respect to Z\n","            alpha -- weighting factors correspond to each class, shape: (C,)\n","            gamma -- modulating factor, a float\n","            \"\"\"\n","\n","            # GRADED FUNCTION: softmax_focalLoss_backward\n","            ## START CODE HERE ### (≈ 10 lines of code)\n","            Z = self.cache\n","            p = np.exp(Z - (b := np.max(Z, axis=1).reshape(-1, 1))) / np.exp(Z - b).sum(axis=1).reshape(-1, 1)\n","            pt = np.full(p.shape, np.max(p * Y, axis=1).reshape(-1, 1))\n","            dZ = np.take(self.alpha, Y.argmax(axis=1)).reshape(-1, 1) * (self.gamma * np.power(1 - pt, self.gamma - 1) * np.log(pt + 1e-5) * ((Y * pt) - p * pt) - np.power(1 - pt, self.gamma) * (Y - p))\n","            ## END CODE HERE ###\n","\n","            assert (dZ.shape == self.cache.shape)\n","\n","            return dZ"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"5PkLKaFWiWmF"},"source":["### 3.2.1. Activation forward (10%)\n","#### 3.2.1.1. Sigmoid function\n","Sigmoid: $\\sigma(Z) = \\begin{cases}\n","    \\frac{1}{1+e^{-Z}},& \\text{if } Z >= 0\\\\\n","    \\frac{e^{Z}}{1+e^{Z}}, & \\text{otherwise}\n","\\end{cases}$.\n","\n","❗**Important**❗: As you can see, there is an exponential function inside the sigmoid function, so you might encounter an exponential overflow problem when implementing this function. To solve this problem, we use the numerically stable sigmoid function as shown in the equation above.\n","\n","### 3.2.1.2. Softmax function\n","Softmax: $\\sigma(\\vec{Z})_i = \\frac{e^{Z_i-b}}{\\sum_{j=1}^{C} e^{Z_j-b}}$, where $\\vec{Z}$ = input vector, $C$ = number of classes in the multi-class classifier, $b$ is $\\max_{j=1}^{C} Z_j$\n","\n","❗**Important**❗: The naive implementation $\\sigma(\\vec{Z})_i = \\frac{e^{Z_i}}{\\sum_{j=1}^{C} e^{Z_j}}$ is terrible when there are large numbers! You might encounter the following problems if you use the naive implementation.\n","*   RuntimeWarning: overflow encountered in exp\n","\n","\n","### 3.2.1.3. ReLU (rectified linear unit) function\n","ReLU: $ReLU(Z) = \\max(Z, 0)$\n","\n","**Exercise**: Implement activation function. (5% + 5%) (basic: Sigmoid and ReLU, advanced: Softmax)\n","\n","**Instruction**:\n","*   Sigmoid: This function returns one item and stores one item: the activation value \"a\" and a cache contains \"z\" (it's what we will use in to the corresponding backward function).\n","*   Softmax: This function returns one item and stores one item: the activation value \"a\" and a cache contains \"z\" (it's what we will use in to the corresponding backward function).\n","*   ReLU: This function returns one item and stores one item: the activation value \"a\" and a cache contains \"z\" (it's what we will use in to the corresponding backward function)."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"gBuRAoeUC5jV"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sigmoid: A = [[0.00669285]\n"," [0.26894142]\n"," [0.5       ]\n"," [0.73105858]\n"," [0.99330715]]\n","ReLU: A = [[0]\n"," [0]\n"," [0]\n"," [1]\n"," [5]]\n","Softmax: A = \n","[[0.0320586  0.08714432 0.23688282 0.64391426]\n"," [0.1748777  0.47536689 0.1748777  0.1748777 ]\n"," [0.0320586  0.08714432 0.23688282 0.64391426]]\n"]}],"source":["Z = np.array([[-5], [-1], [0], [1], [5]])\n","\n","sigmoid = Activation(\"sigmoid\", 'cross_entropy')\n","A = sigmoid.forward(Z)\n","print(\"Sigmoid: A = \" + str(A))\n","A = sigmoid.forward(np.array([[0.23], [-0.67], [0.45], [0.89], [-0.10]]))\n","outputs[\"sigmoid\"] = (A, sigmoid.cache)\n","\n","relu = Activation(\"relu\", 'cross_entropy')\n","A = relu.forward(Z)\n","print(\"ReLU: A = \" + str(A))\n","A = relu.forward(np.array([[-0.34], [-0.76], [0.21], [-0.98], [0.54]]))\n","outputs[\"relu\"] = (A, relu.cache)\n","\n","Z = np.array([[1, 2, 3, 4],[0, 1, 0, 0],[-2, -1, 0, 1]])\n","softmax = Activation(\"softmax\", 'cross_entropy')\n","A = softmax.forward(Z)\n","print(\"Softmax: A = \\n\" + str(A))\n","A = softmax.forward(np.array([[0.12, -0.56, 0.78, -0.34], [0.45, 0.67, -0.89, 0.23], [-0.14, 0.50, -0.76, 0.98]]))\n","outputs[\"softmax\"] = (A, softmax.cache)"]},{"cell_type":"markdown","metadata":{"id":"HyyX_xxdEmNp"},"source":["Expected output:\n","<table>\n","  <tr>\n","    <td>Sigmoid: A</td>\n","    <td>[[0.00669285] [0.26894142] [0.5] [0.73105858] [0.99330715]]</td>\n","  </tr>\n","  <tr>\n","    <td>ReLU: A</td>\n","    <td>[[0] [0] [0] [1] [5]]</td>\n","  </tr>\n","  <tr>\n","    <td>Softmax: A</td>\n","    <td>\n","      [[0.0320586 0.08714432 0.23688282 0.64391426]\n","       [0.1748777 0.47536689 0.1748777 0.1748777]\n","       [0.0320586 0.08714432 0.23688282 0.64391426]]\n","    </td>\n","  </tr>\n","</table>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0tlaPl8PpcbE"},"source":["### 3.2.2. Activation backward (15%)\n","Next, you will need to implement the backward functions of `sigmoid()`, `relu()`, `softmax()`+`compute_CCE_loss` and `softmax()`+`compute_focal_loss`.\n","\n","**Exercise**: Implement backward function. (5% + 10%) (basic: Sigmoid and ReLU, advanced: Softmax+CCE_loss, Softmax+Focal_loss)\n","\n","**Instruction**:\n","*   sigmoid_backward: Implements the backward propagation for SIGMOID unit.\n","*   relu_backward: Implements the backward propagation for RELU unit.\n","*   softmax_CCE_backward: Implements the backward propagation for [SOFTMAX->CCE_LOSS] unit.\n","*   softmax_focal_backward: Implements the backward propagation for [SOFTMAX->Focal_LOSS] unit.\n","\n","If $g(.)$ is the activation function, sigmoid_backward, relu_backward and softmax_backward compute$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$\n","\n","1. The derivative of the sigmoid function is: $$σ^{'}(Z^{[l]}) = σ(Z^{[l]}) (1 - σ(Z^{[l]}))$$. <br>\n","❗**Important**❗: You should use the numerically stable sigmoid function to prevent the overflow exponential problem.\n","\n","2. The derivative of the relu function is: $$g'(Z^{[l]}) = \\begin{cases}\n","    1,& \\text{if } Z^{[l]}> 0\\\\\n","    0,              & \\text{otherwise}\n","\\end{cases}$$\n","\n","3. TLDR😉: The derivative of the softmax + categorical cross-entropy loss with respect to the last hidden layer is: $$\\frac{\\partial \\mathcal{L}}{\\partial Z} = s - y $$. <br> The derivative of the softmax function is: $$\\frac{\\partial S(z_i)}{\\partial z_j} = \\begin{cases}\n","    S(z_i) \\times (1 - S(z_i)),& \\text{if } i = j\\\\\n","    -S(z_i) \\times S(z_j),              & \\text{if } i \\neq j\n","\\end{cases}$$, where $z$ is a vector with shape (number of classes K, 1) and $S(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$. Hence, the real derivative of softmax function would be a full Jacobian matrix. For the special case, K = 4, we have <img src=\"https://miro.medium.com/max/554/1*SWfgFQLDIPXDf1C6CHmr8A.png\" height=\"100\"/>. <br> It is quite complicated to calculate the softmax derivative on its own. However, if you use the softmax and the cross entropy loss, that complexity fades away. Since the softmax layer is usually used at the output, we can actually calculate the derivative of the categorical cross-entropy loss with respect to the n-th node in the last hidden layer. Instead of a long clunky formula, you end up with this terse, easy to compute thing: $$\\frac{\\partial \\mathcal{L}}{\\partial Z_i} = s_i - y_i $$, where $s$ is the output of the softmax function and the $y$ is the true label vector(one-hot vector). For more information, you can refer to this article [Derivative of the Softmax Function and the Categorical Cross-Entropy Loss](https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1). <br>\n","❗**Important**❗: The above mathematical derivation is based on naive implementation. In order to deal with the exponential overflow problem, we should use the normalized exponential function when counting $s$. For the sake of simplicity, we just use the same gradient equation as the naive implementation.\n","\n","4. The derivative of the softmax + Focal loss with respect to the last hidden layer can be written **elementwisely** in the following form:\n","$$\n","\\begin{equation}\n","\\frac{\\partial \\mathcal{L}}{\\partial Z_{ij}} =\n","\\left\\{\n","\\begin{array}{ll}\n","\\alpha_{t} (\\gamma(1 - p_{it})^{\\gamma-1} \\cdot (\\log p_{it}) \\cdot (p_{it} - p_{it}^2) - (1 - p_{it})^\\gamma \\cdot (1 - p_{it})), & j = t \\\\\n","\\alpha_{t} (\\gamma(1 - p_{it})^{\\gamma-1} \\cdot (\\log p_{it}) \\cdot (-p_{ij} \\cdot p_{it}) - (1 - p_{it})^\\gamma \\cdot (-p_{ij})), & j \\neq t\n","\\end{array}\n","\\right.\n","\\end{equation}\n","$$, where\n","- $p$: $a^{[l]}$, that is, the softmax of the $Z^{[l]}$\n","- $p_{ij}$: for the $i$-th example, the predicted probability for the $j$-th class\n","- $p_{it}$: for the $i$-th example, the predicted probability of the true label. (eg. Suppose a predicted probability for the $i$-th example in 3-class classification is $[0.1, 0.4, 0.5]$ and the true label is $[0, 1, 0]$, then the $p_{it}$ is $0.4$)\n","- $\\alpha_{t}$: the $\\alpha$ correspond to the true label. (eg. With the above example, if your $\\alpha$ is $[1,2,3]$, then $\\alpha_{t} = 2$)\n","- $\\gamma$: the modulating factor, a float\n","    \n","   ❗**Important**❗:\n","    * You should add $\\epsilon = 1e-5$ in each $log$.\n","    * You can try to implement it in matrix form to speed it up!\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"0p1wxIeBpcbF"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sigmoid: dZ = [[-0.5       ]\n"," [-0.26935835]\n"," [-0.11969269]\n"," [-0.5       ]\n"," [-0.73139639]]\n","ReLU: dZ = [[-0.    1.7 ]\n"," [-0.    0.  ]\n"," [-1.14  3.72]]\n","SoftmaxCCE: dZ = [[-0.96488097  0.70538451  0.25949646]\n"," [ 0.09003057 -0.75527153  0.66524096]\n"," [ 0.01766842  0.01766842 -0.03533684]]\n","SoftmaxFocal: dZ = [[-1.11727728e+00  8.16795141e-01  3.00482140e-01]\n"," [ 1.96405971e-01 -1.64766071e+00  1.45125474e+00]\n"," [ 1.96157485e-04  1.96157485e-04 -3.92314970e-04]]\n"]}],"source":["dA, cache = np.array([[-2], [-1.37], [-1.14], [-2], [-3.72]]), np.array([[0], [1], [2], [0], [1]])\n","sigmoid = Activation(\"sigmoid\", 'cross_entropy')\n","sigmoid.cache = cache\n","dZ = sigmoid.backward(dA=dA)\n","print(\"Sigmoid: dZ = \"+ str(dZ))\n","dA, cache = np.array([[9.73], [-7.56], [8.34], [-4.12], [6.89]]), np.array([[-5.45], [3.68], [-2.32], [4.51], [-9.27]])\n","sigmoid.cache = cache\n","outputs[\"sigmoid_backward\"] = sigmoid.backward(dA=dA)\n","\n","relu = Activation(\"relu\", 'cross_entropy')\n","dA, cache = np.array([[-2., 1.7 ], [-1.37, 2.], [-1.14, 3.72]]), np.array([[-2, 1], [-1, 0], [2, 1]])\n","relu.cache = cache\n","dZ = relu.backward(dA=dA)\n","print(\"ReLU: dZ = \"+ str(dZ))\n","dA, cache = np.array([[7.24, -3.58], [8.93, 6.45], [-2.11, 9.87]]), np.array([[-4.76, 5.34], [1.98, -7.22], [3.67, -8.56]])\n","relu.cache = cache\n","outputs[\"relu_backward\"] = relu.backward(dA=dA)\n","\n","Y, cache = np.array([[1, 0, 0],[0, 1, 0],[0, 0, 1]]), np.array([[-2, 1, 0],[-1, 0, 1],[-2, -2, 2]])\n","softmax = Activation(\"softmax\", 'cross_entropy')\n","softmax.cache = cache\n","dZ = softmax.backward(Y=Y)\n","print(\"SoftmaxCCE: dZ = \" + str(dZ))\n","Y, cache = np.array([[0, 1, 0], [0, 1, 0], [1, 0, 0]]), np.array([[-9.45, 7.32, 3.58], [5.61, -8.27, 6.49], [1.23, -4.56, 7.84]])\n","softmax.cache = cache\n","outputs[\"softmax_CCE_backward\"] = softmax.backward(Y=Y)\n","\n","Y, cache, alpha, gamma = np.array([[1, 0, 0],[0, 1, 0],[0, 0, 1]]), np.array([[-2, 1, 0],[-1, 0, 1],[-2, -2, 2]]), np.array([1,2,3]), 2.\n","softmax = Activation(\"softmax\", 'focal_loss', alpha, gamma)\n","softmax.cache = cache\n","dZ = softmax.backward(Y=Y)\n","print(\"SoftmaxFocal: dZ = \" + str(dZ))\n","Y, cache, alpha, gamma = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 1]]), np.array([[-6.39, 8.20, -1.54], [5.61, -3.78, 7.32], [-4.56, 9.87, -2.68]]), np.array([0.1,0.2,0.5]), 5\n","softmax.cache = cache\n","outputs[\"softmax_Focal_backward\"] = softmax.backward(Y=Y)"]},{"cell_type":"markdown","metadata":{"id":"OwYDe3WfpcbF"},"source":["Expected output:\n","<table>\n","  <tr>\n","    <td>(with Sigmoid) dZ</td>\n","    <td>[[-0.5] [-0.26935835] [-0.11969269] [-0.5] [-0.73139639]]</td>\n","  </tr>\n","  <tr>\n","    <td>(with ReLU) dZ</td>\n","    <td>[[0 1.7] [0 0] [-1.14 3.72]]</td>\n","  </tr>\n","  <tr>\n","    <td>(with SoftmaxCCE) dZ</td>\n","    <td>\n","      [[-0.96488097 0.70538451 0.25949646]\n","       [0.09003057 -0.75527153 0.66524096]\n","       [0.01766842 0.01766842 -0.03533684]]\n","    </td>\n","  </tr>\n","  <tr>\n","    <td>(with SoftmaxFocal) dZ</td>\n","    <td>\n","      [[-1.11727728e+00 8.16795141e-01 3.00482140e-01]\n","       [1.96405971e-01 -1.64766071e+00 1.45125474e+00]\n","       [1.96157485e-04 1.96157485e-04 -3.92314970e-04]]\n","    </td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"RYqpQu6Eye7h"},"source":["## 3.3. Model (10%)\n","Alright, now you have all the tools that are needed to build a model. Let's get started! 😀\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"0JGMzfIDCSVz"},"outputs":[],"source":["class Model():\n","    def __init__(self, units, activation_functions, loss_function, alpha=None, gamma=None):\n","        self.units = units\n","        self.activation_functions = activation_functions\n","        self.loss_function = loss_function\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.initialize_parameters()\n","\n","    def initialize_parameters(self):\n","        \"\"\"\n","        Arguments:\n","        self.units -- number of nodes/units for each layer, starting from the input dimension and ending with the output dimension (i.e., [4, 4, 1])\n","        self.activation_functions -- activation functions used in each layer (i.e, [\"relu\", \"sigmoid\"])\n","        self.loss_function -- [\"cross_entropy\", \"focal_loss\"]\n","        self.alpha -- weighting factors used by focal loss correspond to each class, shape: (C,)\n","        self.gamma -- a float, used by focal loss\n","        \"\"\"\n","        self.linear = []        # a list to store the dense layers when initializing the model\n","        self.activation = []    # a list to store the activation function layers when initializing the model\n","\n","        for i in range(len(self.units)-1):\n","            dense = Dense(self.units[i], self.units[i+1], i)\n","            self.linear.append(dense)\n","\n","        for i in range(len(self.activation_functions)):\n","            self.activation.append(Activation(self.activation_functions[i], self.loss_function, self.alpha, self.gamma))\n","\n","    def forward(self, X):\n","        \"\"\"\n","        Arguments:\n","        X -- input data: shape (n, f)\n","\n","        Returns:\n","        A -- output of L-layer neural network, probability vector corresponding to your label predictions, shape (n, C)\n","        \"\"\"\n","        A = X\n","\n","        # GRADED FUNCTION: model_forward\n","        ### START CODE HERE ### (≈ 4 lines of code)\n","        for l, a in zip(self.linear, self.activation):\n","            A = a.forward(l.forward(A))\n","        ### END CODE HERE ###\n","\n","        return A\n","\n","    def backward(self, AL=None, Y=None):\n","        \"\"\"\n","        Arguments:\n","        For multi-class classification,\n","        AL -- output of L-layer neural network, probability vector corresponding to your label predictions, shape (n, C)\n","        Y -- true \"label\" vector (one hot vector, for example: [1,0,0] represents rock, [0,1,0] represents paper, [0,0,1] represents scissors\n","                                      in a Rock-Paper-Scissors, shape: (n, C)\n","\n","        Returns:\n","        dA_prev -- post-activation gradient\n","        \"\"\"\n","\n","        L = len(self.linear)\n","        C = Y.shape[1]\n","\n","        # assertions\n","        warning = 'Warning: only the following 4 combinations are allowed! \\n \\\n","                    1. binary classification: sigmoid + cross_entropy) \\n \\\n","                    2. binary classification: softmax + focal_loss) \\n \\\n","                    3. multi-class classification: softmax + cross_entropy) \\n \\\n","                    4. multi-class classification: softmax + focal_loss)'\n","        assert self.loss_function in [\"cross_entropy\", \"focal_loss\"], \"you're using undefined loss function!\"\n","        if Y.shape[1] <= 2:                                 # in binary classification\n","            if self.loss_function == \"cross_entropy\":\n","                assert self.activation_functions[-1] == 'sigmoid', warning\n","                assert self.units[-1] == 1, \"you should set last dim to 1 when using sigmoid + cross_entropy in binary classification!\"\n","            elif self.loss_function  == \"focal_loss\":\n","                assert self.activation_functions[-1] == 'softmax', warning\n","                assert self.units[-1] == 2, \"you should set last dim to 2 when using softmax + focal_loss in binary classification!\"\n","        else:                                               # in multi-class classification\n","            assert self.activation_functions[-1] == 'softmax', warning\n","            assert self.units[-1] == Y.shape[1], f\"you should set last dim to {Y.shape[1]}(the number of classes) in multi-class classification!\"\n","\n","        # GRADED FUNCTION: model_backward\n","        ### START CODE HERE ### (≈ 20 lines of code)\n","\n","        if self.activation_functions[-1] == \"sigmoid\":\n","            if self.loss_function == 'cross_entropy':\n","                # Initializing the backpropagation\n","                dAL = -np.divide(Y, AL + 1e-5) + np.divide(1 - Y, 1 - AL + 1e-5)\n","\n","                # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL\". Outputs: \"dA_prev\"\n","                dZ = self.activation[-1].backward(dA=dAL)\n","                dA_prev = self.linear[-1].backward(dZ)\n","        elif self.activation_functions[-1] == \"softmax\":\n","            # Initializing the backpropagation\n","            dZ = self.activation[-1].backward(Y=Y)\n","\n","            # Lth layer (LINEAR) gradients. Inputs: \"dZ\". Outputs: \"dA_prev\"\n","            dA_prev = self.linear[-1].backward(dZ)\n","\n","        # Loop from l=L-2 to l=0\n","        # lth layer: (RELU -> LINEAR) gradients.\n","        # Inputs: \"dA_prev\". Outputs: \"dA_prev\"\n","        for a, l in zip(self.activation[-2::-1], self.linear[-2::-1]):\n","            dZ = a.backward(dA=dA_prev)\n","            dA_prev = l.backward(dZ)\n","        ### END CODE HERE ###\n","\n","        return dA_prev\n","\n","    def update(self, learning_rate):\n","        \"\"\"\n","        Arguments:\n","        learning_rate -- step size\n","        \"\"\"\n","\n","        L = len(self.linear)\n","\n","        # GRADED FUNCTION: model_update_parameters\n","        ### START CODE HERE ### (≈ 2 lines of code)\n","        for l in self.linear:\n","            l.update(learning_rate)\n","        ### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"xEFH6EffwB0f"},"source":["### 3.3.1. Model initialize parameters (0%)\n","First, you will need to initialize your model by creating several linear and activation function layers.\n","\n","**Exercise**: Implement model initialize parameters.\n","\n","**Instruction**:\n","*   Use the functions you had previously written.\n","*   Store all the linear layers in a list called linear.\n","*   Store all the activation function layers in a list called activation.\n","\n","❗**Important**❗: We set the random seed for grading purposes to keep all the random function calls consistent. However, we still want all the linear layers to have different initialized weights, so when implementing this function, please make sure that you pass the number of iterations as the seed number to the Dense layer initialization call.\n","\n","**Note**: In deep learning, a linear-activation layer is counted as a single layer in the neural network, not two layers since the activation layer does not have any parameter."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"EGY7_1bjcm-c"},"outputs":[{"name":"stdout","output_type":"stream","text":["W1:  [[ 0.09762701  0.08976637 -0.12482558]\n"," [ 0.43037873 -0.1526904   0.783546  ]\n"," [ 0.20552675  0.29178823  0.92732552]] \n","b1:  [[0. 0. 0.]]\n","W2:  [[-0.20325375]\n"," [ 0.53968259]\n"," [-1.22446471]] \n","b2:  [[0.]]\n"]}],"source":["model = Model([3, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n","print(\"W1: \", model.linear[0].parameters[\"W\"], \"\\nb1: \", model.linear[0].parameters[\"b\"])\n","print(\"W2: \", model.linear[1].parameters[\"W\"], \"\\nb2: \", model.linear[1].parameters[\"b\"])"]},{"cell_type":"markdown","metadata":{"id":"LEmggOxtdMnl"},"source":["Expected output:\n","<table>\n","  <tr>\n","    <td>W1:</td>\n","    <td>[[ 0.09762701 0.08976637 -0.12482558] [ 0.43037873 -0.1526904 0.783546 ] [ 0.20552675 0.29178823 0.92732552]]</td>\n","  </tr>\n","  <tr>\n","    <td>b1:</td>\n","    <td>[[0. 0. 0.]]</td>\n","  </tr>\n","  <tr>\n","    <td>W2:</td>\n","    <td>[[-0.20325375] [ 0.53968259] [-1.22446471]]</td>\n","  </tr>\n","  <tr>\n","    <td>b2:</td>\n","    <td>[[0.]]</td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"pJVlZeyNAu-y"},"source":["### 3.3.2. Model forward (4%)\n","\n","After that, you will implement the model forward function by calling the forward function of each layer in the linear and activation function layer you have created in the previous step.\n","\n","For a $N$-layer neural network, you will call the forward function of the linear layers and then followed by the activation function layers for $N-1$ times. The last activation function layer will be sigmoid for binary classification and softmax for multi-class classification.\n","\n","**Exercise**: Implement model forward.\n","\n","**Instruction**:\n","*   Use the functions you had previously written.\n","*   Use a for loop to replicate [LINEAR->ACTIVATION] (N-1) times.\n","\n","**Note**: In the final layer, there are K nodes for K-class classification, but only one node is needed for binary classification. This might seem confusing at first, as it intuitively appears that two nodes should be present in the last layer for binary classification. However, both approaches - the one-node technique (eg. using sigmoid activation + binary cross-entropy loss) and the two-node technique (eg. using softmax activation + categorical cross-entropy loss) - are effective for binary classification. The choice between these techniques is often based on personal preference. For this assignment, in the binary classification, you will use the former approach which is the more common practice, and the latter approach with focal loss (not CCE), due to its ease of implementation.\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"6yVQQqe2EyHA"},"outputs":[{"name":"stdout","output_type":"stream","text":["With sigmoid: A = [[0.64565631]\n"," [0.20915937]\n"," [0.77902611]]\n","With ReLU: A = [[0.6 ]\n"," [0.  ]\n"," [1.26]]\n","With softmax: A = \n","[[0.47535001 0.14317267 0.38147732]\n"," [0.05272708 0.75380161 0.19347131]\n"," [0.68692136 0.05526942 0.25780921]]\n"]}],"source":["A_prev, W, b = np.array([[0.1, 1.1, 2.9],[-1.2, 0.2, -2.5],[1.9, 2.3, 3.7]]), np.array([[0.1], [0.2], [0.3]]), np.array([[-0.5]])\n","model = Model([3, 1], [\"sigmoid\"], \"cross_entropy\")\n","model.linear[0].parameters = {\"W\": W, \"b\": b}\n","A = model.forward(A_prev)\n","print(\"With sigmoid: A = \" + str(A))\n","A_prev, W, b = np.array([[4.35, -5.67], [-7.89, 8.12]]), np.array([[-3.54], [-2.34]]), np.array([[0.8]])\n","model = Model([2, 1], [\"sigmoid\"], \"cross_entropy\")\n","model.linear[0].parameters = {\"W\": W, \"b\": b}\n","A = model.forward(A_prev)\n","outputs[\"model_forward_sigmoid\"] = (A, (model.linear[0].cache, model.activation[0].cache))\n","\n","A_prev, W, b = np.array([[0.1, 1.1, 2.9],[-1.2, 0.2, -2.5],[1.9, 2.3, 3.7]]), np.array([[0.1], [0.2], [0.3]]), np.array([[-0.5]])\n","model = Model([3, 1], [\"relu\"], \"cross_entropy\")\n","model.linear[0].parameters = {\"W\": W, \"b\": b}\n","A = model.forward(A_prev)\n","print(\"With ReLU: A = \" + str(A))\n","A_prev, W, b = np.array([[7.23, -4.56], [5.67, -8.90]]), np.array([[-9.12], [3.45]]), np.array([[0.25]])\n","model = Model([2, 1], [\"relu\"], \"cross_entropy\")\n","model.linear[0].parameters = {\"W\": W, \"b\": b}\n","A = model.forward(A_prev)\n","outputs[\"model_forward_relu\"] = (A, (model.linear[0].cache, model.activation[0].cache))\n","\n","A_prev, W, b = np.array([[0.1, 1.1, 2.9],[-1.2, 0.2, -2.5],[1.9, 2.3, 3.7]]), np.array([[0.1, -0.1, -0.1],[0.2, -0.2, 0.],[0.3, -0.3, 0.1]]), np.array([[-0.5, 0.5, 0.1]])\n","model = Model([3, 3], [\"softmax\"], \"cross_entropy\")\n","model.linear[0].parameters = {\"W\": W, \"b\": b}\n","A = model.forward(A_prev)\n","print(\"With softmax: A = \\n\" + str(A))\n","A_prev, W, b = np.array([[-5.12, 4.56, 7.89], [8.34, -6.78, 2.45], [3.21, -4.67, 5.98]]), np.array([[6.23, -7.85, 4.56], [-3.21, 9.87, -2.34], [1.23, -5.67, 8.90]]), np.array([[4.12, -6.54, 7.89]])\n","model = Model([3, 3], [\"softmax\"], \"cross_entropy\")\n","model.linear[0].parameters = {\"W\": W, \"b\": b}\n","A = model.forward(A_prev)\n","outputs[\"model_forward_softmax\"] = (A, (model.linear[0].cache, model.activation[0].cache))"]},{"cell_type":"markdown","metadata":{"id":"QMkf2ss6F52W"},"source":["Expected output:\n","<table>\n","  <tr>\n","    <td>With Sigmoid:</td>\n","    <td>A = [[0.64565631] [0.20915937] [0.77902611]]</td>\n","  </tr>\n","  <tr>\n","    <td>With ReLU:</td>\n","    <td>A = [[0.6 ] [0. ] [1.26]]</td>\n","  </tr>\n","  <tr>\n","    <td>With Softmax:</td>\n","    <td>A = [[0.47535001 0.14317267 0.38147732] [0.05272708 0.75380161 0.19347131] [0.68692136 0.05526942 0.25780921]]</td>\n","  </tr>\n","</table>"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"s26LVkCbIbJ3"},"outputs":[{"name":"stdout","output_type":"stream","text":["AL = [[0.56058713]\n"," [0.55220559]\n"," [0.46331713]]\n","Length of layers list = 2\n","AL = [[0.11637212 0.08186754 0.0924809  0.09675205 0.12819411 0.09664001\n","  0.08448599 0.09067641 0.1294968  0.08303407]\n"," [0.11413265 0.08432761 0.09365443 0.09736489 0.12404237 0.09726785\n","  0.08664355 0.09207969 0.12512634 0.08536063]\n"," [0.09750771 0.07419482 0.08444682 0.10943351 0.09669465 0.11116299\n","  0.08734059 0.12452515 0.13002144 0.08467232]]\n","Length of layers list = 2\n"]}],"source":["# binary classification\n","X = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]])\n","model = Model([3, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n","AL = model.forward(X)\n","print(\"AL = \" + str(AL))\n","print(\"Length of layers list = \" + str(len(model.linear)))\n","\n","# multi-class classification\n","X = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]])\n","model = Model([3, 3, 10], [\"relu\", \"softmax\"], \"cross_entropy\")\n","AL = model.forward(X)\n","print(\"AL = \" + str(AL))\n","print(\"Length of layers list = \" + str(len(model.linear)))"]},{"cell_type":"markdown","metadata":{"id":"zoCdrONOHhvw"},"source":["Expected output:\n","<table>\n","  <tr>\n","    <td>AL:</td>\n","    <td>[[0.56058713] [0.55220559] [0.46331713]]</td>\n","  </tr>\n","  <tr>\n","    <td>Length of layers list:</td>\n","    <td>2</td>\n","  </tr>\n","  <tr>\n","    <td>AL:</td>\n","    <td>[[0.11637212 0.08186754 0.0924809  0.09675205 0.12819411 0.09664001 0.08448599 0.09067641 0.1294968  0.08303407]\n","         [0.11413265 0.08432761 0.09365443 0.09736489 0.12404237 0.09726785 0.08664355 0.09207969 0.12512634 0.08536063]\n","         [0.09750771 0.07419482 0.08444682 0.10943351 0.09669465 0.11116299 0.08734059 0.12452515 0.13002144 0.08467232]]</td>\n","  </tr>\n","  <tr>\n","    <td>Length of layers list:</td>\n","    <td>2</td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"hPBl7iq7N2wY"},"source":["###3.3.3. Model backward (4%)\n","Now you will implement the backward function for the whole network. Recall that you have implemented the backward function for the dense and activation function layer. In this section, you will call these functions to help you implement the model backward function. You will iterate through all the hidden layers backward, starting from layer $L$. On each step, you will call the backward function of layer $l$ to backpropagate through layer $l$.\n","\n","**Exercise**: Implement model backward.\n","\n","**Instruction**:\n","*   Use the functions you had previously written.\n","*   Initialize backpropagation.\n","*   Use a for loop to backprop from layer $L-1$ to layer $1$.\n","\n","Initializing backpropagation:\n","\n","(1) **Binary classification**: To backpropagate through this network, we know that the output is, $A^{[L]} = \\sigma(Z^{[L]})$. Your code thus needs to compute dAL $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$. To do so, use the formulas (derived using calculus which you don't need in-depth knowledge of):\n","```\n","dAL = - (np.divide(Y, AL + ϵ) - np.divide(1 - Y, 1 - AL + ϵ)) # derivative of loss with respect to AL, where ϵ = 1e-5 is added to prevent zero division.\n","```\n","You can then use this post-activation gradient dAL to keep going backward. You can now feed in dAL into the LINEAR->SIGMOID backward function you implemented (which will use the cached values stored inside each layer in the forward pass). After that, you will have to use a for loop to iterate through all the other layers using the LINEAR->RELU backward function.\n","\n","(2) **Multi-class classification**: Since you have implemented the backward function of the softmax activation function layer along with the categorical cross-entropy loss (same with focal loss), you can directly call the softmax_CCE_backward function implemented inside the activation function layer and followed by the linear backward function to obtain the post-activation gradient to keep going backward. After that, you will have to use a for loop to iterate through all the other layers using the LINEAR->RELU backward function.\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"HOGsyLXPNGh5"},"outputs":[{"name":"stdout","output_type":"stream","text":["sigmoid:\n","dA_prev = [[ 0.55554938  0.27777469]\n"," [ 0.49152369  0.24576184]\n"," [-0.41996594 -0.20998297]\n"," [-0.55554938 -0.27777469]\n"," [-0.39321993 -0.19660997]]\n","dW = [[-0.29446117]\n"," [ 0.29446117]]\n","db = [[-0.03216622]]\n","\n","relu:\n","dA_prev = [[-0.01269296 -0.05595562]\n"," [ 0.01470136  0.06480946]\n"," [ 0.          0.        ]\n"," [-0.07496777 -0.0327431 ]\n"," [-0.07151883 -0.03123674]]\n","dW = [[ 0.0178719  -0.17321413]\n"," [-0.0178719   0.17321413]]\n","db = [[ 0.00335943 -0.11638953]]\n","\n"]}],"source":["AL, Y, linear_activation_cache = np.array([[0.1], [0.2], [0.5], [0.9], [1.0]]), np.array([[0], [0], [1], [1], [1]]), (((np.array([[-2, 2], [-1, 1], [0, 0], [1, -1], [2, -2]]), np.array([[2.0], [1.0]]), np.array([[0.5]])), np.array([[0], [1], [2], [0], [1]])))\n","model = Model([2, 1], [\"sigmoid\"], \"cross_entropy\")\n","model.linear[0].cache = linear_activation_cache[0]\n","model.activation[0].cache = linear_activation_cache[1]\n","dA_prev = model.backward(AL=AL, Y=Y)\n","print (\"sigmoid:\")\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(model.linear[0].dW))\n","print (\"db = \" + str(model.linear[0].db) + \"\\n\")\n","AL, Y, linear_activation_cache = np.array([[0.35], [0.93], [0.23], [0.72], [0.90]]), np.array([[1], [0], [1], [0], [1]]), (((np.array([[-1, 2], [1, 3], [2, 0], [1, -4], [3, -2]]), np.array([[1.7], [3.2]]), np.array([[0.25]])), np.array([[2], [1], [2], [0], [0]])))\n","model = Model([2, 1], [\"sigmoid\"], \"cross_entropy\")\n","model.linear[0].cache = linear_activation_cache[0]\n","model.activation[0].cache = linear_activation_cache[1]\n","dA_prev = model.backward(AL=AL, Y=Y)\n","outputs[\"model_backward_sigmoid\"] = (dA_prev, model.linear[0].dW, model.linear[0].db)\n","\n","X, Y = np.array([[-2, 2], [-1, 1], [0, 0], [1, -1], [2, -2]]), np.array([[0], [1], [1], [1], [1]])\n","model = Model([2, 2, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n","AL = model.forward(X)\n","dA_prev = model.backward(AL=AL, Y=Y)\n","print (\"relu:\")\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(model.linear[0].dW))\n","print (\"db = \" + str(model.linear[0].db) + \"\\n\")\n","X, Y = np.array([[4.56, -3.21], [-7.85, 6.34], [2.45, -8.90], [5.67, 3.12], [-4.78, 7.89]]), np.array([[1], [1], [0], [1], [0]])\n","model = Model([2, 2, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n","AL = model.forward(X)\n","dA_prev = model.backward(AL=AL, Y=Y)\n","outputs[\"model_backward_relu\"] = (dA_prev, model.linear[0].dW, model.linear[0].db)"]},{"cell_type":"markdown","metadata":{"id":"o6xzEk3-NGh6"},"source":["Expected output:\n","<table>\n","  <tr>\n","    <th colspan=\"2\">Sigmoid</th>\n","  </tr>\n","  <tr>\n","    <td>dA_prev:</td>\n","    <td>[[ 0.55554938  0.27777469] [ 0.49152369  0.24576184] [-0.41996594 -0.20998297] [-0.55554938 -0.27777469] [-0.39321993 -0.19660997]]</td>\n","  </tr>\n","  <tr>\n","    <td>dW:</td>\n","    <td>[[-0.29446117] [ 0.29446117]]</td>\n","  </tr>\n","  <tr>\n","    <td>db:</td>\n","    <td>[[-0.03216622]]</td>\n","  </tr>\n","  <tr>\n","    <th colspan=\"2\">ReLU</th>\n","  </tr>\n","  <tr>\n","    <td>dA_prev:</td>\n","    <td>[[-0.01269296 -0.05595562] [ 0.01470136  0.06480946] [ 0.  0. ] [-0.07496777 -0.0327431 ] [-0.07151883 -0.03123674]]</td>\n","  </tr>\n","  <tr>\n","    <td>dW:</td>\n","    <td>[[ 0.0178719  -0.17321413] [-0.0178719   0.17321413]]</td>\n","  </tr>\n","  <tr>\n","    <td>db:</td>\n","    <td>[[ 0.00335943 -0.11638953]]</td>\n","  </tr>\n","</table>"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"BC1QnMSKN2wZ"},"outputs":[{"name":"stdout","output_type":"stream","text":["Binary classification - cross_entropy\n","dW1 = [[-0.06277946  0.26602938 -0.37820327]\n"," [ 0.          0.05875647  0.        ]\n"," [-0.01569486  0.05181823 -0.09455082]]\n","db1 = [[-0.03138973  0.10363646 -0.18910163]]\n","dA_prev = [[-0.02128713  0.03620889 -0.06919444]\n"," [ 0.02675119 -0.04550313  0.08695554]\n"," [ 0.08406585 -0.52321654 -0.47247201]]\n","\n","Binary classification - focal_loss\n","dW1 = [[ 0.16650099  1.26851068 -0.133887  ]\n"," [ 0.         -0.33320702  0.        ]\n"," [ 0.04162525  0.40042943 -0.03347175]]\n","db1 = [[ 0.08325049  0.80085885 -0.0669435 ]]\n","dA_prev = [[-0.01381806  0.02350419 -0.04491602]\n"," [ 0.11736847 -0.19964092  0.3815097 ]\n"," [ 0.16157143 -0.24058586  0.22954524]]\n","\n","Multi-class classification - cross_entropy\n","dW1 = [[ 0.16593371  0.33171007 -0.32297709]\n"," [ 0.          0.15006987  0.        ]\n"," [ 0.04148343  0.04541005 -0.08074427]]\n","db1 = [[ 0.08296685  0.0908201  -0.16148854]]\n","dA_prev = [[-0.04735391  0.08054785 -0.15392528]\n"," [ 0.05429414 -0.09235301  0.1764847 ]\n"," [ 0.10229066 -0.30227651 -0.34116033]]\n","\n","Multi-class classification - focal_loss\n","dW1 = [[ 0.47080109  0.79334211 -0.9163778 ]\n"," [ 0.         -0.16669457  0.        ]\n"," [ 0.11770027  0.24000917 -0.22909445]]\n","db1 = [[ 0.23540055  0.48001834 -0.4581889 ]]\n","dA_prev = [[-0.03467556  0.05898229 -0.11271394]\n"," [ 0.11424182 -0.19432256  0.37134643]\n"," [ 0.29022768 -0.85764439 -0.96796884]]\n","\n"]}],"source":["# binary classification - cross_entropy\n","X, Y = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]]), np.array([[1], [0], [0]])\n","model = Model([3, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n","AL = model.forward(X)\n","dA_prev = model.backward(AL=AL, Y=Y)\n","print(\"Binary classification - cross_entropy\")\n","print(\"dW1 = \"+ str(model.linear[0].dW))\n","print(\"db1 = \"+ str(model.linear[0].db))\n","print(\"dA_prev = \"+ str(dA_prev) +\"\\n\")\n","\n","# binary classification - focal loss\n","X, Y = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]]), np.array([[1, 0], [0, 1], [0, 1]])\n","model = Model([3, 3, 2], [\"relu\", \"softmax\"], \"focal_loss\", alpha=np.array([1,2]), gamma=2.)\n","AL = model.forward(X)\n","dA_prev = model.backward(AL=AL, Y=Y)\n","print(\"Binary classification - focal_loss\")\n","print(\"dW1 = \"+ str(model.linear[0].dW))\n","print(\"db1 = \"+ str(model.linear[0].db))\n","print(\"dA_prev = \"+ str(dA_prev) +\"\\n\")\n","\n","# multi-class classification - cross_entropy\n","X, Y = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]]), np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n","model = Model([3, 3, 3], [\"relu\", \"softmax\"], \"cross_entropy\")\n","AL = model.forward(X)\n","dA_prev = model.backward(AL=AL, Y=Y)\n","print(\"Multi-class classification - cross_entropy\")\n","print(\"dW1 = \"+ str(model.linear[0].dW))\n","print(\"db1 = \"+ str(model.linear[0].db))\n","print(\"dA_prev = \"+ str(dA_prev) +\"\\n\")\n","\n","# multi-class classification - focal_loss\n","X, Y = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]]), np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n","model = Model([3, 3, 3], [\"relu\", \"softmax\"], \"focal_loss\", alpha=np.array([1,2,3]), gamma=2.)\n","AL = model.forward(X)\n","dA_prev = model.backward(AL=AL, Y=Y)\n","print(\"Multi-class classification - focal_loss\")\n","print(\"dW1 = \"+ str(model.linear[0].dW))\n","print(\"db1 = \"+ str(model.linear[0].db))\n","print(\"dA_prev = \"+ str(dA_prev) +\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"9cYzCzY8N2wZ"},"source":["Expected output:\n","<table>\n","  <tr>\n","    <th colspan=\"2\">Binary Classification - Cross Entropy</th>\n","  </tr>\n","  <tr>\n","    <td>dW1:</td>\n","    <td>[[-0.06277946  0.26602938 -0.37820327] [ 0.  0.05875647  0. ] [-0.01569486  0.05181823 -0.09455082]]</td>\n","  </tr>\n","  <tr>\n","    <td>db1:</td>\n","    <td>[[-0.03138973  0.10363646 -0.18910163]]</td>\n","  </tr>\n","  <tr>\n","    <td>dA_prev:</td>\n","    <td>[[-0.02128713  0.03620889 -0.06919444] [ 0.02675119 -0.04550313  0.08695554] [ 0.08406585 -0.52321654 -0.47247201]]</td>\n","  </tr>\n","  <tr>\n","    <th colspan=\"2\">Binary Classification - Focal Loss</th>\n","  </tr>\n","  <tr>\n","    <td>dW1:</td>\n","    <td>[[ 0.16650099  1.26851068 -0.133887 ] [ 0. -0.33320702  0. ] [ 0.04162525  0.40042943 -0.03347175]]</td>\n","  </tr>\n","  <tr>\n","    <td>db1:</td>\n","    <td>[[ 0.08325049  0.80085885 -0.0669435 ]]</td>\n","  </tr>\n","  <tr>\n","    <td>dA_prev:</td>\n","    <td>[[-0.01381806  0.02350419 -0.04491602] [ 0.11736847 -0.19964092  0.3815097 ] [ 0.16157143 -0.24058586  0.22954524]]</td>\n","  </tr>\n","  <tr>\n","    <th colspan=\"2\">Multi-class Classification - Cross Entropy</th>\n","  </tr>\n","  <tr>\n","    <td>dW1:</td>\n","    <td>[[ 0.16593371  0.33171007 -0.32297709] [ 0.  0.15006987  0. ] [ 0.04148343  0.04541005 -0.08074427]]</td>\n","  </tr>\n","  <tr>\n","    <td>db1:</td>\n","    <td>[[ 0.08296685  0.0908201  -0.16148854]]</td>\n","  </tr>\n","  <tr>\n","    <td>dA_prev:</td>\n","    <td>[[-0.04735391  0.08054785 -0.15392528] [ 0.05429414 -0.09235301  0.1764847 ] [ 0.10229066 -0.30227651 -0.34116033]]</td>\n","  </tr>\n","  <tr>\n","    <th colspan=\"2\">Multi-class Classification - Focal Loss</th>\n","  </tr>\n","  <tr>\n","    <td>dW1:</td>\n","    <td>[[ 0.47080109  0.79334211 -0.9163778 ] [ 0.  -0.16669457  0. ] [ 0.11770027  0.24000917 -0.22909445]]</td>\n","  </tr>\n","  <tr>\n","    <td>db1:</td>\n","    <td>[[ 0.23540055  0.48001834 -0.4581889 ]]</td>\n","  </tr>\n","  <tr>\n","    <td>dA_prev:</td>\n","    <td>[[-0.03467556  0.05898229 -0.11271394] [ 0.11424182 -0.19432256  0.37134643] [ 0.29022768 -0.85764439 -0.96796884]]</td>\n","  </tr>\n","</table>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"5wiJu3YlUCc7"},"source":["### 3.3.4. Model update parameters (2%)\n","In this section you will update the parameters of the model, using gradient descent:\n","\n","$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} $$\n","$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} $$\n","where $\\alpha$ is the learning rate.\n","\n","**Exercise**: Implement update() to update your parameters using gradient descent.\n","\n","**Instructions**:\n","*   Use the functions you had previously written.\n","*   Update parameters using gradient descent on every $W^{[l]}$ and $b^{[l]}$ for $l = 1, 2, ..., L$.\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"qoGA4O8BUCvq"},"outputs":[{"name":"stdout","output_type":"stream","text":["W1 = [[ 0.39721186  0.07752363  0.392862  ]\n"," [ 0.64025004  0.00469968  0.52183369]\n"," [-0.09671178  0.09679955  0.33138026]\n"," [ 0.27099015  0.33705631  0.67538482]]\n","b1 = [[ 0.16234149  0.78232848 -0.02592894]]\n","W2 = [[0.6012798 ]\n"," [0.38575324]\n"," [0.49003974]]\n","b2 = [[0.05692437]]\n"]}],"source":["np.random.seed(1)\n","parameters, grads = {\"W1\": np.random.rand(3, 4).T, \"b1\": np.random.rand(3,1).T, \"W2\": np.random.rand(1,3).T, \"b2\": np.random.rand(1,1).T}, {\"dW1\": np.random.rand(3, 4).T, \"db1\": np.random.rand(3,1).T, \"dW2\": np.random.rand(1,3).T, \"db2\": np.random.rand(1,1).T}\n","model = Model([4, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n","model.linear[0].parameters = {\"W\": parameters[\"W1\"], \"b\": parameters[\"b1\"]}\n","model.linear[1].parameters = {\"W\": parameters[\"W2\"], \"b\": parameters[\"b2\"]}\n","model.linear[0].dW, model.linear[0].db, model.linear[1].dW, model.linear[1].db = grads[\"dW1\"], grads[\"db1\"], grads[\"dW2\"], grads[\"db2\"]\n","model.update(0.1)\n","print (\"W1 = \"+ str(model.linear[0].parameters[\"W\"]))\n","print (\"b1 = \"+ str(model.linear[0].parameters[\"b\"]))\n","print (\"W2 = \"+ str(model.linear[1].parameters[\"W\"]))\n","print (\"b2 = \"+ str(model.linear[1].parameters[\"b\"]))\n","\n","np.random.seed(1)\n","parameters, grads = {\"W1\": np.random.rand(3, 4).T, \"b1\": np.random.rand(3,1).T, \"W2\": np.random.rand(1,3).T, \"b2\": np.random.rand(1,1).T}, {\"dW1\": np.random.rand(3, 4).T, \"db1\": np.random.rand(3,1).T, \"dW2\": np.random.rand(1,3).T, \"db2\": np.random.rand(1,1).T}\n","model = Model([4, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n","model.linear[0].parameters = {\"W\": parameters[\"W1\"], \"b\": parameters[\"b1\"]}\n","model.linear[1].parameters = {\"W\": parameters[\"W2\"], \"b\": parameters[\"b2\"]}\n","model.linear[0].dW, model.linear[0].db, model.linear[1].dW, model.linear[1].db = grads[\"dW1\"], grads[\"db1\"], grads[\"dW2\"], grads[\"db2\"]\n","model.update(0.075)\n","outputs[\"model_update_parameters\"] = {\"W1\": model.linear[0].parameters[\"W\"], \"b1\": model.linear[0].parameters[\"b\"], \"W2\": model.linear[1].parameters[\"W\"], \"b2\": model.linear[1].parameters[\"b\"]}"]},{"cell_type":"markdown","metadata":{"id":"9t-HfnHZWYIa"},"source":["Expected output:\n","<table>\n","  <tr>\n","    <th colspan=\"2\">Data Representation</th>\n","  </tr>\n","  <tr>\n","    <td>W1:</td>\n","    <td>[[ 0.39721186  0.07752363  0.392862 ] [ 0.64025004  0.00469968  0.52183369] [-0.09671178  0.09679955  0.33138026] [ 0.27099015  0.33705631  0.67538482]]</td>\n","  </tr>\n","  <tr>\n","    <td>b1:</td>\n","    <td>[[ 0.16234149  0.78232848 -0.02592894]]</td>\n","  </tr>\n","  <tr>\n","    <td>W2:</td>\n","    <td>[[0.6012798 ] [0.38575324] [0.49003974]]</td>\n","  </tr>\n","  <tr>\n","    <td>b2:</td>\n","    <td>[[0.05692437]]</td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"SmSBVaQOSRrk"},"source":["# 4. Loss function (15%)\n","In this section, you will implement the loss function. We use binary cross-entropy loss for binary classification and categorical cross-entropy loss for multi-class classification. You need to compute the loss, because you want to check if your model is actually learning. Cross-entropy loss is minimized, where smaller values represent a better model than larger values. A model that predicts perfect probabilities has a cross entropy or log loss of 0.0.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ScdQdj85uC0P"},"source":["## 4.1. Binary cross-entropy loss (5%)\n","**Exercise**: Compute the binary cross-entropy loss $L$, using the following formula:  $$-\\frac{1}{n} \\sum\\limits_{i = 1}^{n} (y^{(i)}\\log\\left(a^{[L] (i)}+ϵ\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}+ϵ\\right)), where\\ ϵ=1e-5$$"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"MjBT0eYQaY81"},"outputs":[],"source":["# GRADED FUNCTION: compute_BCE_loss\n","\n","def compute_BCE_loss(AL, Y):\n","    \"\"\"\n","    Implement the binary cross-entropy loss function using the above formula.\n","\n","    Arguments:\n","    AL -- probability vector corresponding to your label predictions, shape (n, 1)\n","    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (n, 1)\n","\n","    Returns:\n","    loss -- binary cross-entropy loss\n","    \"\"\"\n","\n","    n = Y.shape[0]\n","\n","    # Compute loss from aL and y.\n","    ### START CODE HERE ### (≈ 1 line of code)\n","    loss = (-1 / n) * (Y * np.log(AL + 1e-5) + (1 - Y) * (np.log(1 - AL + 1e-5))).sum()\n","    ### END CODE HERE ###\n","\n","    loss = np.squeeze(loss)      # To make sure your loss's shape is what we expect (e.g. this turns [[17]] into 17).\n","    assert(loss.shape == ())\n","\n","    return loss"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"r07sqnIXaaMv"},"outputs":[{"name":"stdout","output_type":"stream","text":["loss = 0.5783820772863568\n"]}],"source":["AL, Y = np.array([[0.9], [0.6], [0.4], [0.1], [0.2], [0.8]]), np.array([[1], [1], [1], [0], [0], [0]])\n","\n","print(\"loss = \" + str(compute_BCE_loss(AL, Y)))\n","outputs[\"compute_BCE_loss\"] = compute_BCE_loss(np.array([[0.12], [0.85], [0.47], [0.33], [0.76], [0.58], [0.09], [0.62]]), np.array([[1], [1], [0], [1], [0], [1], [1], [0]]))"]},{"cell_type":"markdown","metadata":{"id":"4iRtgOx_IGPo"},"source":["Expected output:\n","<table>\n","  <tr>\n","    <td>loss: </td>\n","    <td>0.5783820772863568</td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"aealRyKbcQzG"},"source":["## 4.2. Categorical cross-entropy loss (CCE) (5%)\n","**Exercise**:\n","Compute the categorical cross-entropy loss $L$, using the following formula: $$-\\frac{1}{n} \\sum\\limits_{i = 1}^{n} (y^{(i)}\\log\\left(a^{[L] (i)}+ϵ\\right)),\\ ϵ = 1e-5$$\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"Owx-kTdcfxV5"},"outputs":[],"source":["# GRADED FUNCTION: compute_CCE_loss\n","\n","def compute_CCE_loss(AL, Y):\n","    \"\"\"\n","    Implement the categorical cross-entropy loss function using the above formula.\n","\n","    Arguments:\n","    AL -- probability vector corresponding to your label predictions, shape (n, C)\n","    Y -- true \"label\" vector (one hot vector, for example: [1,0,0] represents rock, [0,1,0] represents paper, [0,0,1] represents scissors\n","                                      in a Rock-Paper-Scissors, shape: (n, C)\n","\n","    Returns:\n","    loss -- categorical cross-entropy loss\n","    \"\"\"\n","\n","    n = Y.shape[0]\n","\n","    # Compute loss from aL and y.\n","    ### START CODE HERE ### (≈ 1 line of code)\n","    loss = (-1 / n) * (Y * np.log(AL + 1e-5)).sum().sum()\n","    ### END CODE HERE ###\n","\n","    loss = np.squeeze(loss)      # To make sure your loss's shape is what we expect (e.g. this turns [[17]] into 17).\n","    assert(loss.shape == ())\n","\n","    return loss"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"0YbHVAc7hSh3"},"outputs":[{"name":"stdout","output_type":"stream","text":["loss = 0.4722526144672341\n"]}],"source":["AL, Y = np.array([[0.8, 0.1, 0.1],[0.6, 0.3, 0.1],[0.4, 0.5, 0.1],[0.1, 0.7, 0.2],[0.2, 0.1, 0.7],[0.4, 0.1, 0.5]]), np.array([[1, 0, 0],[1, 0, 0],[0, 1, 0],[0, 1, 0],[0, 0, 1],[0, 0, 1]])\n","print(\"loss = \" + str(compute_CCE_loss(AL, Y)))\n","outputs[\"compute_CCE_loss\"] = compute_CCE_loss(np.array([[0.7, 0.2, 0.1], [0.2, 0.2, 0.6], [0.3, 0.5, 0.2], [0.8, 0.1, 0.1], [0.7, 0.15, 0.15]]), np.array([[1, 0, 0], [0, 0, 0], [0, 1, 0], [0, 0, 0], [0, 0, 1]]))"]},{"cell_type":"markdown","metadata":{"id":"p9VVIBB5Ic-D"},"source":["Expected output:\n","<table>\n","  <tr>\n","    <td>loss: </td>\n","    <td>0.4722526144672341</td>\n","  </tr>\n","</table>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8wJ5Ial2tzsl"},"source":["## 4.3. Focal loss (5%)\n","**Exercise**:\n","If we think of the CCE loss elementwisely, the above equation can be re-written in the form below:\n","$$-\\frac{1}{n} \\sum\\limits_{i = 1}^{n} ( \\sum\\limits_{j = 1}^{c} (y_{ij}\\log\\left(a^{[L]}_{ij}+ϵ\\right))), \\ ϵ = 1e-5$$\n","\n","To handle the imbalance dataset, we can use the focal loss, which adds the weighting factor $\\alpha$ and the modulating term $(1-a^{[L]}_{ij})^\\gamma$. To compute the focal loss, you can use the following equation:\n","$$-\\frac{1}{n} \\sum\\limits_{i = 1}^{n} (\\sum\\limits_{j = 1}^{c} (\\alpha_{j} (1-a^{[L]}_{ij})^\\gamma * y_{ij}\\log\\left(a^{[L]}_{ij}+ϵ\\right))),$$\n","where\n","* $\\ ϵ = 1e-5$\n","* $\\alpha$ is the weighting factors in the shape $(c,)$, where ${\\alpha}_i$ corresponds to the class $i$\n","* $\\gamma$ is a modulating factor\n","* $n$ is the number of examples\n","* $c$ is the number of classes\n","\n","Since $y_i$ would be a one-hot vector, we can further simplify the equation to\n","$$\n","\\text{Focal Loss} =\n","-\\frac{1}{n} \\sum_{i=1}^{n} \\alpha_t (1 - p_{it})^\\gamma \\log(p_{it}+ϵ)\n","$$\n","where\n","* $p$: $a^{[L]}$\n","* $p_{ij}$: for the $i$-th example, the predicted probability for the $j$-th class\n","* $p_{it}$: for the $i$-th example, the predicted probability of the true label. (eg. Suppose a predicted probability for the $i$-th example in 3-class classification is $[0.1, 0.4, 0.5]$ and the true label is $[0, 1, 0]$, then the $p_{it}$ is $0.4$)\n","* $\\alpha_{t}$: the $\\alpha$ correspond to the true label. (eg. With the above example, if your $\\alpha$ is $[1,2,3]$, then $\\alpha_{t} = 2$)\n","\n","**Note**: Since this equation computes the focal loss elementwisely, you can try to make it in the matrix form to speed up the computation!"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"99UXtihhuIZc"},"outputs":[],"source":["def compute_focal_loss(AL, Y, alpha, gamma):\n","\n","    # Compute loss from aL and y.\n","    ### START CODE HERE ### (10 line of code)\n","    loss = (-1 / Y.shape[0]) * (np.take(alpha, Y.argmax(axis=1)) * np.power(1 - (pt := np.max(AL * Y, axis=1)), gamma) * np.log(pt + 1e-5)).sum()\n","    ### END CODE HERE ###\n","\n","    loss = np.squeeze(loss)\n","    assert(loss.shape == ())\n","\n","    return loss"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"y1vbihGUqyGv"},"outputs":[{"name":"stdout","output_type":"stream","text":["loss = 0.252813549602297\n"]}],"source":["AL, Y, alpha, gamma = np.array([[0.5, 0.3, 0.2], [0.1, 0.1, 0.8]]), np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]), np.array([4.0, 0.22, 3.0]), 4.0\n","print(\"loss = \" + str(compute_focal_loss(AL, Y, alpha=alpha, gamma=gamma)))\n","outputs[\"compute_focal_loss\"] = compute_focal_loss(np.array([[0.7, 0.2, 0.1], [0.2, 0.2, 0.6], [0.3, 0.5, 0.2], [0.8, 0.1, 0.1], [0.7, 0.15, 0.15]]), np.array([[1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1]]), np.array([1.0, 2.0, 3.0]), 2.0)"]},{"cell_type":"markdown","metadata":{"id":"nZOLXwbwUqm_"},"source":["Expected output:\n","<table>\n","  <tr>\n","    <td>loss: </td>\n","    <td>0.252813549602297</td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"mpQah0JDdMyl"},"source":["# Basic implementation (binary classification) (20%)\n","\n","Congratulations on implementing all the functions by yourself. You've done an incredible job! 👏\n","\n","Now, you have all the tools necessary to begin the classification. In this section, you'll build a binary classifier using the functions you previously wrote. Our goal is to use some patients' health information and predict whether their condition is worse than a given threshold after a period of time. The features have been preprocessed using min-max normalization and the data has been shuffled.\n","\n","**Exercise**: Implement a binary classifier and tune hyperparameter.\n","\n","**Instruction**:\n","* Use the functions you had previously written.\n","* You can try these two combinations for your last activation + loss function\n","    1. sigmoid + cross_entropy\n","    2. softmax + focal_loss\n","    \n","**Note**: More commonly, we set the output dimension to 1 and use the sigmoid function for binary classification. However, for simplicity in implementing focal loss, we treat binary classification as a multi-class classification. As a result, you first need to transform 'y' from a single label to a one-hot label. (eg.[[1], [0], [1], [0]] -> [[0, 1], [1, 0], [0, 1], [1, 0]]) Secondly, you must set your output dimension to 2 when using focal loss.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QpFQpiK5eF64"},"source":["## Helper function"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"woCqucFUYXe6"},"outputs":[],"source":["def predict(x, y_true, model):\n","    \"\"\"\n","    This function is used to predict the results of a  L-layer neural network.\n","\n","    Arguments:\n","    x -- data set of examples you would like to label\n","    model -- trained model\n","\n","    Returns:\n","    y_pred -- predictions for the given dataset X\n","    \"\"\"\n","\n","    n = x.shape[0]\n","\n","    # Forward propagation\n","    y_pred = model.forward(x)\n","\n","    # this transform the output and label of binary classification when using sigmoid + cross entropy for evaluation\n","    # eg. y_pred: [[0.8], [0.2], [0.1]] -> [[0.2, 0.8], [0.8, 0.2], [0.9, 0.1]]\n","    # eg. y_true: [[1], [0], [0]] -> [[0, 1], [1, 0], [1, 0]]\n","    if y_pred.shape[-1] == 1:\n","        y_pred = np.array([[1 - y[0], y[0]] for y in y_pred])\n","        if y_true is not None:\n","            y_true = np.array([[1,0] if y == 0 else [0,1] for y in y_true.reshape(-1)])\n","\n","    # make y_pred/y_true become one-hot prediction result\n","    # eg. y_true: [[1, 0, 0], [0, 0, 1], [0, 1, 0]] -> [0, 2, 1]\n","    # eg. y_pred: [[0.2, 0.41, 0.39], [0.1, 0.8, 0.1], [0.1, 0.1, 0.8]] -> [1, 1, 2]\n","    if y_true is not None:\n","        y_true = np.argmax(y_true, axis=1)\n","    y_pred = np.argmax(y_pred, axis=1)\n","\n","    if y_true is not None:\n","        # compute accuracy\n","        correct = 0\n","        for yt, yp in zip(y_true, y_pred):\n","            if yt == yp:\n","                correct += 1\n","        print(f\"Accuracy: {correct/n * 100:.2f}%\")\n","\n","        f1_micro = f1_score(y_true, y_pred, average=None)\n","        print(f'f1_micro score for each class: {f1_micro}')\n","        print(f'f1_micro score average: {np.mean(np.array(f1_micro)):.2f}')\n","\n","    return y_pred"]},{"cell_type":"markdown","metadata":{"id":"vAvUwG1uSLg_"},"source":["## Read data & train_val split\n","\n","As you can see, the data distribution is imbalanced. Therefore, we can try using focal loss, which is an effective loss function designed to address imbalanced datasets. Let's delve into the the focal loss equation.\n","$$\n","\\text{Focal Loss} =\n","- \\sum_{i=1}^{n} \\alpha_t (1 - p_{it})^\\gamma \\log(p_{it})\n","$$\n","The parameter $\\alpha$ is an array of weighting factors for each class, which can be adjusted to balance the classes. The parameter $\\gamma$ acts as a modulating factor, reducing the loss contribution from well-classified examples. For instance, if $p_{it} = 0.9$, indicating a $90\\%$ confidence that the prediction for the $i$th example is correct, a higher $\\gamma$ value will result in a lower loss. You can adjust $\\alpha$ and $\\gamma$ based on the data distribution shown in the plot below.\n","\n","**Note**: When spliting data, you could try to plot and see if the data distribution is the same in training and validation to ensure the the correctness of your validation."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"Cu8T1DKjb7Bn"},"outputs":[],"source":["try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    prefix = \"/content/drive/MyDrive\"\n","except:\n","    prefix = '.'"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"pQ0xMNj7b-1q"},"outputs":[{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAOYAAADvCAYAAADmZahzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbSElEQVR4nO3deVAUZ/4G8GdQB5AzKIcoIkZjxAPizRqPNSiiwZgYjUdlQV1PiBJ2jYsnalKIBnUlRtQtJZv1iklU1NUC8Va8V10Ub7wKAUFhABVQ3t8f/uh1HBAYRuZFnk/VVNlvX99u56Gv6W6VEEKAiKRiYuwCiEgXg0kkIQaTSEIMJpGEGEwiCTGYRBJiMIkkxGASSYjBJJIQg1kLBQQEoFmzZlptKpUKYWFhb3zeBw4cgEqlwoEDB5S23r17o23btm983gBw69YtqFQqxMTEVMv89MVgVlBMTAxUKpXyMTMzg7OzM3x8fLB8+XLk5ubqPe1jx44hLCwM2dnZhiu4GmzYsAHLli0zdhmlkrm2iqhr7AJqmvnz58PNzQ1FRUVIS0vDgQMHEBwcjCVLliA2Nhbt27ev9DSPHTuGefPmISAgALa2toYvugKePHmCunUr93XYsGEDkpKSEBwcXOFxevbsiSdPnkCtVleywsopqzZXV1c8efIE9erVe6PzryoGs5J8fX3RqVMnpTs0NBT79u3Dxx9/jEGDBiE5ORnm5uZGrFA/ZmZmb3T6T58+hVqthomJyRuf1+uU7O3IjruyBtCnTx/Mnj0bt2/fxr/+9S+l/cKFCwgICEDz5s1hZmYGJycnjBkzBllZWcowYWFhmDZtGgDAzc1N2VW+desWAGDdunXo06cPHBwcYGpqCnd3d6xcubLCtW3btg1t27aFmZkZ2rZti61bt5Y63KvHmLm5uQgODkazZs1gamoKBwcH9O3bF2fPngXw4rhw165duH37tlJzyXFryXHkpk2bMGvWLDRu3Bj169eHRqMp9RizxJkzZ/CHP/wB5ubmcHNzQ3R0tFb/ksOJknVT4tVpvq62so4x9+3bhx49esDCwgK2trb45JNPkJycrDVMWFgYVCoVrl+/ruzd2NjYYPTo0Xj8+HHZ/wl64BbTQL788kvMmDEDcXFxGDduHAAgPj4eN2/exOjRo+Hk5ISLFy9i9erVuHjxIo4fPw6VSoXPPvsMV69excaNG7F06VI0bNgQAGBvbw8AWLlyJdq0aYNBgwahbt262LFjByZPnozi4mIEBga+tqa4uDgMGTIE7u7uCA8PR1ZWFkaPHo0mTZqUuzwTJ07Er7/+iqCgILi7uyMrKwtHjhxBcnIyOnTogJkzZyInJwf37t3D0qVLAQCWlpZa01iwYAHUajX++te/oqCg4LW7r48ePcKAAQMwbNgwjBgxAr/88gsmTZoEtVqNMWPGlFvvyypS28v27t0LX19fNG/eHGFhYXjy5AmioqLQvXt3nD17VudE2bBhw+Dm5obw8HCcPXsW//jHP+Dg4ICIiIhK1flagipk3bp1AoA4depUmcPY2NiIDz74QOl+/PixzjAbN24UAMShQ4eUtsWLFwsAIiUlRWf40qbh4+MjmjdvXm7Nnp6eolGjRiI7O1tpi4uLEwCEq6ur1rAAxNy5c7WWJTAw8LXTHzhwoM50hBBi//79AoBo3ry5Tv0l/fbv36+09erVSwAQkZGRSltBQYHw9PQUDg4OorCwUAjxv/+DV9dTadMsq7aUlBQBQKxbt05pK5lPVlaW0nb+/HlhYmIi/vSnPyltc+fOFQDEmDFjtKb56aefigYNGujMqyq4K2tAlpaWWmdnXz7WfPr0KTIzM9GtWzcAUHYJy/PyNHJycpCZmYlevXrh5s2byMnJKXO8+/fv49y5c/D394eNjY3S3rdvX7i7u5c7X1tbW5w4cQKpqakVqrM0/v7+FT7erlu3LiZMmKB0q9VqTJgwARkZGThz5ozeNZSnZD0FBATAzs5OaW/fvj369u2Lf//73zrjTJw4Uau7R48eyMrKgkajMVhdDKYB5eXlwcrKSul++PAhpk6dCkdHR5ibm8Pe3h5ubm4A8NpQvezo0aPw9vZWjn3s7e0xY8aMcqdx+/ZtAEDLli11+rVq1arc+S5atAhJSUlwcXFBly5dEBYWhps3b1ao5hIly1oRzs7OsLCw0Gp77733AEDnmNKQStZTaeukdevWyMzMRH5+vlZ706ZNtbrfeecdAC92xw2FwTSQe/fuIScnBy1atFDahg0bhjVr1mDixIn4/fffERcXhz179gAAiouLy53mjRs38NFHHyEzMxNLlizBrl27EB8fj6+//rrC09DXsGHDcPPmTURFRcHZ2RmLFy9GmzZtsHv37gpPw9Bnp1UqVantz58/N+h8ylOnTp1S24UBn9LDkz8G8vPPPwMAfHx8ALz465mQkIB58+Zhzpw5ynDXrl3TGbesL9yOHTtQUFCA2NhYrb/S+/fvL7ceV1fXMud35cqVcscHgEaNGmHy5MmYPHkyMjIy0KFDB3z33Xfw9fV9bd36SE1NRX5+vtZW8+rVqwCgnHwp2TK9+kOMkq3eyypaW8l6Km2dXL58GQ0bNtTZklcHbjENYN++fViwYAHc3NwwatQoAP/7q/rqX9HSfo1S8h//6heutGnk5ORg3bp15dbUqFEjeHp64qefftLa5Y2Pj8elS5deO+7z5891dpMdHBzg7OyMgoICrboruktenmfPnmHVqlVKd2FhIVatWgV7e3t07NgRAPDuu+8CAA4dOqRV6+rVq3WmV9HaXl5PL6//pKQkxMXFYcCAAfouUpVwi1lJu3fvxuXLl/Hs2TOkp6dj3759iI+Ph6urK2JjY5WL19bW1ujZsycWLVqEoqIiNG7cGHFxcUhJSdGZZskXb+bMmRg+fDjq1asHPz8/9OvXD2q1Gn5+fpgwYQLy8vKwZs0aODg44P79++XWGh4ejoEDB+LDDz/EmDFj8PDhQ0RFRaFNmzbIy8src7zc3Fw0adIEn3/+OTw8PGBpaYm9e/fi1KlTiIyM1Kp78+bNCAkJQefOnWFpaQk/P7/KrlIAL44xIyIicOvWLbz33nvYvHkzzp07h9WrVyu/0mnTpg26deuG0NBQPHz4EHZ2dti0aROePXumM73K1LZ48WL4+vrCy8sLY8eOVS6X2NjYVMvvh0tl0HO8b7GSU/UlH7VaLZycnETfvn3F3//+d6HRaHTGuXfvnvj000+Fra2tsLGxEUOHDhWpqak6lyaEEGLBggWicePGwsTEROuSQGxsrGjfvr0wMzMTzZo1ExEREWLt2rVlXl551W+//SZat24tTE1Nhbu7u/j999+Fv7//ay+XFBQUiGnTpgkPDw9hZWUlLCwshIeHh/jxxx+1xsnLyxMjR44Utra2WpdgSi5fbNmyRaeesi6XtGnTRpw+fVp4eXkJMzMz4erqKn744Qed8W/cuCG8vb2FqampcHR0FDNmzBDx8fE60yyrttIulwghxN69e0X37t2Fubm5sLa2Fn5+fuLSpUtaw5RcLnnw4IFWe1mXcapCJQSfK0skGx5jEkmIwSSSEINJJCEGk0hCDCaRhBhMIgnxBwZ48ZvT1NRUWFlZGfRnZkSvEkIgNzcXzs7OMDEpe7vIYOLF7zRdXFyMXQbVInfv3n3tDesMJqDcqnX37l1YW1sbuRp6m2k0Gri4uGjdHlgaBhP/uxPB2tqawaRqUd4hE0/+EEmIwSSSEINJJCEGk0hCPPlTAc3+tsvYJRjFrYUDjV1CrcUtJpGEGEwiCTGYRBJiMIkkxGASSYjBJJIQg0kkIQaTSEIMJpGEGEwiCTGYRBJiMIkkxGASSYjBJJIQg0kkIQaTSEIMJpGEGEwiCTGYRBJiMIkkxGASSYjBJJIQg0kkIQaTSEIMJpGEGEwiCTGYRBJiMIkkxGASSYjBJJKQUYMZHh6Ozp07w8rKCg4ODhg8eDCuXLmiNczTp08RGBiIBg0awNLSEkOGDEF6errWMHfu3MHAgQNRv359ODg4YNq0aXj27Fl1LgqRQRk1mAcPHkRgYCCOHz+O+Ph4FBUVoV+/fsjPz1eG+frrr7Fjxw5s2bIFBw8eRGpqKj777DOl//PnzzFw4EAUFhbi2LFj+OmnnxATE4M5c+YYY5GIDEIlhBDGLqLEgwcP4ODggIMHD6Jnz57IycmBvb09NmzYgM8//xwAcPnyZbRu3RqJiYno1q0bdu/ejY8//hipqalwdHQEAERHR2P69Ol48OAB1Gp1ufPVaDSwsbFBTk4OrK2tdfrzxbVkKOV910pIdYyZk5MDALCzswMAnDlzBkVFRfD29laGef/999G0aVMkJiYCABITE9GuXTsllADg4+MDjUaDixcvljqfgoICaDQarQ+RTKQJZnFxMYKDg9G9e3e0bdsWAJCWlga1Wg1bW1utYR0dHZGWlqYM83IoS/qX9CtNeHg4bGxslI+Li4uBl4aoaqQJZmBgIJKSkrBp06Y3Pq/Q0FDk5OQon7t3777xeRJVRl1jFwAAQUFB2LlzJw4dOoQmTZoo7U5OTigsLER2drbWVjM9PR1OTk7KMCdPntSaXslZ25JhXmVqagpTU1MDLwWR4Rh1iymEQFBQELZu3Yp9+/bBzc1Nq3/Hjh1Rr149JCQkKG1XrlzBnTt34OXlBQDw8vLCf//7X2RkZCjDxMfHw9raGu7u7tWzIEQGZtQtZmBgIDZs2IDt27fDyspKOSa0sbGBubk5bGxsMHbsWISEhMDOzg7W1tb46quv4OXlhW7dugEA+vXrB3d3d3z55ZdYtGgR0tLSMGvWLAQGBnKrSDWWUYO5cuVKAEDv3r212tetW4eAgAAAwNKlS2FiYoIhQ4agoKAAPj4++PHHH5Vh69Spg507d2LSpEnw8vKChYUF/P39MX/+/OpaDCKDk+o6prHwOmbpeB3T8GrkdUwieoHBJJIQg0kkIQaTSEIMJpGEGEwiCTGYRBLSK5h3797FvXv3lO6TJ08iODgYq1evNlhhRLWZXsEcOXIk9u/fD+DFrVV9+/bFyZMnMXPmTP7ihsgA9ApmUlISunTpAgD45Zdf0LZtWxw7dgzr169HTEyMIesjqpX0CmZRUZHyA/G9e/di0KBBAF48XeD+/fuGq46oltIrmG3atEF0dDQOHz6M+Ph49O/fHwCQmpqKBg0aGLRAotpIr2BGRERg1apV6N27N0aMGAEPDw8AQGxsrLKLS0T60+u2r969eyMzMxMajQbvvPOO0j5+/HhYWFgYrDii2kqvLWafPn2Qm5urFUrgxdPtvvjiC4MURlSb6RXMAwcOoLCwUKf96dOnOHz4cJWLIqrtKrUre+HCBeXfly5d0no85PPnz7Fnzx40btzYcNUR1VKVCqanpydUKhVUKhX69Omj09/c3BxRUVEGK46otqpUMFNSUiCEQPPmzXHy5EnY29sr/dRqNRwcHFCnTh2DF0lU21QqmK6urgBePDWdiN4cvZ+Sd+3aNezfvx8ZGRk6QeWbtoiqRq9grlmzBpMmTULDhg3h5OQElUql9FOpVAwmURXpFcxvv/0W3333HaZPn27oeogIel7HfPToEYYOHWroWojo/+kVzKFDhyIuLs7QtRDR/9NrV7ZFixaYPXs2jh8/jnbt2qFevXpa/adMmWKQ4ohqK71ekfDqW7m0JqhS4ebNm1UqqrrxFQml4ysSDK+ir0jQa4uZkpKid2FEVD4+JY9IQnptMceMGfPa/mvXrtWrGCJ6Qa9gPnr0SKu7qKgISUlJyM7OLvXH7URUOXoFc+vWrTptxcXFmDRpEt59990qF0VU2xnsGNPExAQhISFYunSpoSZJVGsZ9OTPjRs38OzZM0NOkqhW0mtXNiQkRKtbCIH79+9j165d8Pf3N0hhRLWZXsH8z3/+o9VtYmICe3t7REZGlnvGlojKp1cwS95bQkRvht43SgPAgwcPcOXKFQBAq1attB41QkT60+vkT35+PsaMGYNGjRqhZ8+e6NmzJ5ydnTF27Fg8fvzY0DUS1Tp6BTMkJAQHDx7Ejh07kJ2djezsbGzfvh0HDx7EX/7yF0PXSFTr6LUr+9tvv+HXX39F7969lbYBAwbA3Nwcw4YNw8qVKw1VH1GtpNcW8/Hjx3B0dNRpd3Bw4K4skQHoFUwvLy/MnTsXT58+VdqePHmCefPmwcvLy2DFEdVWeu3KLlu2DP3790eTJk2UV/CdP38epqamfOQIkQHoFcx27drh2rVrWL9+PS5fvgwAGDFiBEaNGgVzc3ODFkhUG+m1KxseHo5NmzZh3LhxiIyMRGRkJP785z9j48aNiIiIqPB0Dh06BD8/Pzg7O0OlUmHbtm1a/YUQmDNnDho1agRzc3N4e3vj2rVrWsM8fPgQo0aNgrW1NWxtbTF27Fjk5eXps1hE0tArmKtWrcL777+v017yCviKys/Ph4eHB1asWFFq/0WLFmH58uWIjo7GiRMnYGFhAR8fH61j21GjRuHixYuIj4/Hzp07cejQIYwfP77yC0UkEb12ZdPS0tCoUSOddnt7e9y/f7/C0/H19YWvr2+p/YQQWLZsGWbNmoVPPvkEAPDPf/4Tjo6O2LZtG4YPH47k5GTs2bMHp06dQqdOnQAAUVFRGDBgAL7//ns4OzvrsXRExqfXFtPFxQVHjx7VaT969KjBwpCSkoK0tDR4e3srbTY2NujatSsSExMBAImJibC1tVVCCQDe3t4wMTHBiRMnypx2QUEBNBqN1odIJnptMceNG4fg4GAUFRUpjxJJSEjAN998Y7Bf/pS8FPfV66WOjo5Kv7S0NDg4OGj1r1u3Luzs7LReqvuq8PBwzJs3zyB1Er0JegVz2rRpyMrKwuTJk5VXvpuZmWH69OkIDQ01aIFvQmhoqNY9pRqNBi4uLkasiEibXsFUqVSIiIjA7NmzkZycDHNzc7Rs2RKmpqYGK8zJyQkAkJ6ernU8m56eDk9PT2WYjIwMrfGePXuGhw8fKuOXxtTU1KC1EhlalR4tYmlpic6dO6Nt27YG/6K7ubnByckJCQkJSptGo8GJEyeUXxd5eXkhOzsbZ86cUYbZt28fiouL0bVrV4PWQ1SdqnQ/ZlXl5eXh+vXrSndKSgrOnTsHOzs7NG3aFMHBwfj222/RsmVLuLm5Yfbs2XB2dsbgwYMBAK1bt0b//v0xbtw4REdHo6ioCEFBQRg+fDjPyFKNZtRgnj59Gn/84x+V7pLjPn9/f8TExOCbb75Bfn4+xo8fj+zsbHz44YfYs2cPzMzMlHHWr1+PoKAgfPTRRzAxMcGQIUOwfPnyal8W0lVb3/kCVP29L3q9VOhtw5cKla6qX67aut6AstddRV8qxHeXEEmIwSSSEINJJCEGk0hCDCaRhBhMIgkxmEQSYjCJJMRgEkmIwSSSEINJJCEGk0hCDCaRhBhMIgkxmEQSYjCJJMRgEkmIwSSSEINJJCEGk0hCDCaRhBhMIgkxmEQSYjCJJMRgEkmIwSSSEINJJCEGk0hCDCaRhBhMIgkxmEQSYjCJJMRgEkmIwSSSEINJJCEGk0hCDCaRhBhMIgkxmEQSYjCJJMRgEkmIwSSSEINJJCEGk0hCDCaRhN6aYK5YsQLNmjWDmZkZunbtipMnTxq7JCK9vRXB3Lx5M0JCQjB37lycPXsWHh4e8PHxQUZGhrFLI9LLWxHMJUuWYNy4cRg9ejTc3d0RHR2N+vXrY+3atcYujUgvdY1dQFUVFhbizJkzCA0NVdpMTEzg7e2NxMTEUscpKChAQUGB0p2TkwMA0Gg0pQ5fXPDYgBXXHGWtj4qqresNKHvdlbQLIV47fo0PZmZmJp4/fw5HR0etdkdHR1y+fLnUccLDwzFv3jyddhcXlzdSY01ls8zYFdRc5a273Nxc2NjYlNm/xgdTH6GhoQgJCVG6i4uL8fDhQzRo0AAqlcqIlWnTaDRwcXHB3bt3YW1tbexyagyZ15sQArm5uXB2dn7tcDU+mA0bNkSdOnWQnp6u1Z6eng4nJ6dSxzE1NYWpqalWm62t7Zsqscqsra2l+4LVBLKut9dtKUvU+JM/arUaHTt2REJCgtJWXFyMhIQEeHl5GbEyIv3V+C0mAISEhMDf3x+dOnVCly5dsGzZMuTn52P06NHGLo1IL29FML/44gs8ePAAc+bMQVpaGjw9PbFnzx6dE0I1jampKebOnauz202v9zasN5Uo77wtEVW7Gn+MSfQ2YjCJJMRgEkmIwSSSEIMpMd7KVjmHDh2Cn58fnJ2doVKpsG3bNmOXpDcGU1K8la3y8vPz4eHhgRUrVhi7lCrj5RJJde3aFZ07d8YPP/wA4MWvmVxcXPDVV1/hb3/7m5Grk59KpcLWrVsxePBgY5eiF24xJVRyK5u3t7fSVt6tbPR2YTAl9Lpb2dLS0oxUFVUnBpNIQgymhPS5lY3eLgymhHgrG70Vd5e8jXgrW+Xl5eXh+vXrSndKSgrOnTsHOzs7NG3a1IiV6UGQtKKiokTTpk2FWq0WXbp0EcePHzd2SVLbv3+/AKDz8ff3N3ZplcbrmEQS4jEmkYQYTCIJMZhEEmIwiSTEYBJJiMEkkhCDSSQhBpNIQgxmLde7d28EBwdXaNgDBw5ApVIhOzu7SvNs1qwZli1bVqVpvO0YTCIJMZhEEmIwSfHzzz+jU6dOsLKygpOTE0aOHFnqw7+OHj2K9u3bw8zMDN26dUNSUpJW/yNHjqBHjx4wNzeHi4sLpkyZgvz8/OpajLcCg0mKoqIiLFiwAOfPn8e2bdtw69YtBAQE6Aw3bdo0REZG4tSpU7C3t4efnx+KiooAADdu3ED//v0xZMgQXLhwAZs3b8aRI0cQFBRUzUtTwxn79hYyrl69eompU6eW2u/UqVMCgMjNzRVC/O+2qk2bNinDZGVlCXNzc7F582YhhBBjx44V48eP15rO4cOHhYmJiXjy5IkQQghXV1exdOlSwy/MW4RbTFKcOXMGfn5+aNq0KaysrNCrVy8AwJ07d7SGe/kpCnZ2dmjVqhWSk5MBAOfPn0dMTAwsLS2Vj4+PD4qLi5GSklJ9C1PD8QkGBODFw5J9fHzg4+OD9evXw97eHnfu3IGPjw8KCwsrPJ28vDxMmDABU6ZM0elX454iYEQMJgEALl++jKysLCxcuBAuLi4AgNOnT5c67PHjx5WQPXr0CFevXkXr1q0BAB06dMClS5fQokWL6in8LcVdWQLwYmumVqsRFRWFmzdvIjY2FgsWLCh12Pnz5yMhIQFJSUkICAhAw4YNlSeeT58+HceOHUNQUBDOnTuHa9euYfv27Tz5U0kMJgEA7O3tERMTgy1btsDd3R0LFy7E999/X+qwCxcuxNSpU9GxY0ekpaVhx44dUKvVAID27dvj4MGDuHr1Knr06IEPPvgAc+bMgbOzc3UuTo3HZ/4QSYhbTCIJMZhEEmIwiSTEYBJJiMEkkhCDSSQhBpNIQgwmkYQYTCIJMZhEEmIwiST0f2ciHbsvuXdKAAAAAElFTkSuQmCC","text/plain":["<Figure size 200x200 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Train: x=(353, 10), y=(353, 1)\n","Test: x=(89, 10)\n","x_train: (283, 10) | y_train: (283, 1)\n","x_val: (70, 10) | y_val: (70, 1)\n"]}],"source":["# load data\n","data = np.load(f\"{prefix}/basic_data.npz\")\n","X_train, Y_train = data[\"x_train\"], data[\"y_train\"]\n","X_test = data[\"x_test\"]\n","\n","# plot the data distribution\n","Y_train_1 = len(['_' for y in Y_train if y == 1])\n","Y_train_0 = len(['_' for y in Y_train if y == 0])\n","plt.figure(figsize=(2, 2))\n","plt.bar([0, 1], [Y_train_0, Y_train_1])\n","plt.title('Data distribution')\n","plt.xlabel('label')\n","plt.ylabel('counts')\n","plt.show()\n","\n","print('Train: x=%s, y=%s' % (X_train.shape, Y_train.shape))\n","print('Test: x=%s' % (X_test.shape, ))\n","\n","### START CODE HERE ###\n","# train_val split\n","n = X_train.shape[0]\n","pos, _ = np.where(Y_train)\n","neg, _ = np.where(Y_train == 0)\n","np.random.shuffle(pos)\n","np.random.shuffle(neg)\n","x_train, y_train = X_train[indices_train := np.concatenate((pos[len(pos) // 5:], neg[len(neg) // 5:]))], Y_train[indices_train]\n","x_val, y_val = X_train[indices_valid := np.concatenate((pos[:len(pos) // 5], neg[:len(neg) // 5]))], Y_train[indices_valid]\n","### END CODE HERE ###\n","\n","print(\"x_train: \" + str(x_train.shape) + \" | y_train: \" + str(y_train.shape))\n","print(\"x_val: \" + str(x_val.shape) + \" | y_val: \" + str(y_val.shape))"]},{"cell_type":"markdown","metadata":{"id":"r01QzzHxeMbR"},"source":["## Training and Evaluation"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"fI7JY5ESjhZ2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss after iteration 0: 0.181422\n","Loss after iteration 256: 0.178319\n","Loss after iteration 512: 0.176222\n","Loss after iteration 768: 0.174187\n","Loss after iteration 1024: 0.171952\n","Loss after iteration 1280: 0.169328\n","Loss after iteration 1536: 0.166470\n","Loss after iteration 1792: 0.163428\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 600x300 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# GRADED CODE: binary classification\n","### START CODE HERE ###\n","\n","loss_function = \"focal_loss\"\n","if loss_function == 'cross_entropy':\n","    layers_dims = None\n","    activation_fn = None\n","    gamma = None        # you can leave this as it is\n","    alpha = None        # you can leave this as it is\n","    y_train_processed = None\n","    y_val_processed = None\n","    assert y_train_processed.shape[-1] == 1, \"see the 'Note' in the Basic implementation section\"\n","    assert y_val_processed.shape[-1] == 1, \"see the 'Note' in the Basic implementation section\"\n","elif loss_function == 'focal_loss':\n","    layers_dims = [10, 50, 100, 500, 1000, 200, 100, 20, 10, 2]\n","    activation_fn = [\"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"softmax\"]\n","    gamma = math.sqrt(2 * math.pi)\n","    alpha = np.array([1, len(neg) / len(pos)])\n","    y_train_processed = np.eye(2).astype(int)[y_train.reshape(-1)]\n","    y_val_processed = np.eye(2).astype(int)[y_val.reshape(-1)]\n","    assert y_train_processed.shape[-1] == 2, \"see the 'Note' in the Basic implementation section\"\n","    assert y_val_processed.shape[-1] == 2, \"see the 'Note' in the Basic implementation section\"\n","\n","learning_rate = 2.5e-3\n","num_iterations = 2048\n","print_loss = True\n","print_freq = 256\n","classes = 2\n","losses = []                         # keep track of loss\n","model = Model(layers_dims, activation_fn, loss_function, alpha, gamma)\n","\n","# Loop (batch gradient descent)\n","for i in range(num_iterations):\n","    # forward\n","    AL = model.forward(x_train)\n","\n","    # compute loss\n","    if loss_function == 'cross_entropy':\n","        loss = None\n","    elif loss_function == 'focal_loss':\n","        loss = compute_focal_loss(AL, y_train_processed, alpha, gamma)\n","\n","    # backward\n","    dA_prev = model.backward(AL, y_train_processed)\n","\n","    # update\n","    model.update(learning_rate)\n","\n","    losses.append(loss)\n","    if print_loss and i % print_freq == 0:\n","        print (\"Loss after iteration %i: %f\" %(i, loss))\n","\n","# plot the loss\n","plt.figure(figsize=(6, 3))\n","plt.plot(np.squeeze(losses))\n","plt.ylabel('loss')\n","plt.xlabel(f'iterations (per {print_freq})')\n","plt.title(\"Learning rate =\" + str(learning_rate))\n","plt.show()\n","### END CODE HERE ###"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"U8q0a20XcPtk"},"outputs":[{"name":"stdout","output_type":"stream","text":["training------\n","Accuracy: 72.79%\n","f1_micro score for each class: [0.7890411  0.61691542]\n","f1_micro score average: 0.70\n","validation------\n","Accuracy: 72.86%\n","f1_micro score for each class: [0.79120879 0.6122449 ]\n","f1_micro score average: 0.70\n"]}],"source":["print('training------')\n","pred_train = predict(x_train, y_train_processed, model)\n","print('validation------')\n","pred_val = predict(x_val, y_val_processed, model)"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"mERo3g41zsyX"},"outputs":[],"source":["pred_test = predict(X_test, None, model)\n","outputs[\"basic_pred_test\"] = pred_test\n","outputs[\"basic_layers_dims\"] = layers_dims\n","outputs[\"basic_activation_fn\"] = activation_fn\n","outputs[\"basic_loss_function\"] = loss_function\n","outputs[\"basic_alpha\"] = alpha\n","outputs[\"basic_gamma\"] = gamma\n","basic_model_parameters = []\n","for basic_linear in model.linear:\n","    basic_model_parameters.append(basic_linear.parameters)\n","outputs[\"basic_model_parameters\"] = basic_model_parameters"]},{"cell_type":"markdown","metadata":{"id":"oMCpPFMVdj36"},"source":["# Advanced implementation (multi class classification) (15%)\n","\n","In this section, you need to implement a multi-class classifier using the functions you had previously written. You will create a model that can classify ten handwritten digits. The MNIST handwritten digit classification problem is a standard dataset in computer vision and deep learning. We usually use convolutional deep-learning neural networks for image classification. However, using only dense layers appears to be enough to handle this simple dataset, and this is a good way to get started with image datasets.\n","\n","**Exercise**: Implement a multi-class classifier and tune hyperparameter.\n","\n","**Instruction**:\n","*   Use the functions you had previously written.\n","*   Preprocess the data to match the correct input format.\n","*   Use mini-batch gradient descent to train the model.\n","\n","**Hint**:\n","For data preprocessing, please be careful with the dimension of the inputs (X and y) and also note that the values of images are usually integers that fall between 0 and 255. You need to change the data type into float and scale the values between 0 and 1.\n","\n","In Batch Gradient Descent, we consider all the samples for every step of Gradient Descent. But what if our dataset is huge? You will get around 33000 training samples, then to take one step, the model will have to calculate the gradients of all the 33000 samples. This does not seem an efficient way. Hence, mini-batch gradient descent is recommended to be used in this part."]},{"cell_type":"markdown","metadata":{"id":"I_GQ3uO128OC"},"source":["## Read data & train_val split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ew4u79ZuV5F"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bVSfqnXqXGdC"},"outputs":[],"source":["# load data\n","data = np.load('/content/drive/MyDrive/advanced_data.npz')\n","X_train = data[\"x_train\"]\n","Y_train = data[\"y_train\"]\n","X_test = data[\"x_test\"]\n","\n","# summarize loaded dataset\n","print(f'Train: X={X_train.shape}, Y={Y_train.shape}')\n","print(f'Test: X={X_test.shape}')\n","# plot first few images\n","for i in range(9):\n","\t# define subplot\n","\tplt.subplot(330 + 1 + i)\n","\t# plot raw pixel data\n","\tplt.imshow(X_train[i], cmap='gray', vmin=0, vmax=255)\n","# show the figure\n","plt.show()\n","\n","# GRADED CODE: multi-class classification (Data preprocessing) one-hot encoding for y\n","### START CODE HERE ###\n","Y_train = None\n","### END CODE HERE ###\n","\n","print(\"shape of X_train: \" + str(X_train.shape))\n","print(\"shape of Y_train: \" + str(Y_train.shape))\n","print(\"shape of X_test: \" + str(X_test.shape))\n","\n","# GRADED CODE: multi-class classification (Data preprocessing)\tnormalize x\n","### START CODE HERE ###\n","X_train = None\n","X_test = None\n","### END CODE HERE ###\n","\n","print(\"\\nshape of X_train: \" + str(X_train.shape))\n","print(\"shape of Y_train: \" + str(Y_train.shape))\n","print(\"shape of X_test: \" + str(X_test.shape))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ljAcf2tpQDR-"},"outputs":[],"source":["# You can split training and validation set here and visualize their distribution. (Optional)\n","# If not, just leave this as it is\n","### START CODE HERE ###\n","x_train = X_train\n","y_train = Y_train\n","x_val = None\n","y_val = None\n","\n","print(\"shape of x_train: \" + str(x_train.shape))\n","print(\"shape of y_train: \" + str(y_train.shape))\n","### END CODE HERE ###\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ngmUDGN13ADi"},"source":["## Training and Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HYD-qRs7doU0"},"outputs":[],"source":["# GRADED CODE: multi-class classification\n","### START CODE HERE ###\n","def random_mini_batches(X, Y, mini_batch_size = 64):\n","    \"\"\"\n","    Creates a list of random minibatches from (X, Y)\n","\n","    Arguments:\n","    X -- input data, of shape (n, f^{0})\n","    Y -- true \"label\" vector, of shape (n, C)\n","    mini_batch_size -- size of the mini-batches, integer\n","\n","    Returns:\n","    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n","    \"\"\"\n","\n","    m = X.shape[0]                  # number of training examples\n","    mini_batches = []\n","\n","    # Step 1: Shuffle (X, Y)\n","    permutation = list(np.random.permutation(m))\n","    shuffled_X = None\n","    shuffled_Y = None\n","\n","    # Step 2 - Partition (shuffled_X, shuffled_Y).\n","    # Cases with a complete mini batch size only i.e each of 64 examples.\n","    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n","    for k in range(0, num_complete_minibatches):\n","        # (approx. 2 lines)\n","        mini_batch_X = None\n","        mini_batch_Y = None\n","        mini_batch = (mini_batch_X, mini_batch_Y)\n","        mini_batches.append(mini_batch)\n","\n","    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n","    if m % mini_batch_size != 0:\n","        #(approx. 2 lines)\n","        mini_batch_X = None\n","        mini_batch_Y = None\n","        mini_batch = (mini_batch_X, mini_batch_Y)\n","        mini_batches.append(mini_batch)\n","\n","    return mini_batches\n","\n","\n","layers_dims = None\n","activation_fn = None\n","learning_rate = None\n","num_iterations = None\n","batch_size = None\n","classes = 10\n","losses = []                         # keep track of loss\n","print_loss = True\n","print_freq = 100\n","loss_function = None\n","gamma = None\n","alpha = None\n","model = Model(layers_dims, activation_fn, loss_function, alpha=alpha, gamma=gamma)\n","\n","# Loop (gradient descent)\n","for i in range(0, num_iterations):\n","    mini_batches = random_mini_batches(x_train, y_train, batch_size)\n","    loss = 0\n","    for batch in mini_batches:\n","        x_batch, y_batch = batch\n","\n","        # forward\n","        AL = None\n","\n","        # compute loss\n","        if loss_function == 'cross_entropy':\n","            loss += None\n","        elif loss_function == 'focal_loss':\n","            loss += None\n","\n","        # backward\n","        dA_prev = None\n","        # update\n","        None\n","\n","    loss /= len(mini_batches)\n","    losses.append(loss)\n","    if print_loss and i % print_freq == 0:\n","        print (\"Loss after iteration %i: %f\" %(i, loss))\n","\n","\n","# plot the loss\n","plt.figure(figsize=(4, 2))\n","plt.plot(np.squeeze(losses))\n","plt.ylabel('loss')\n","plt.xlabel('iterations (per hundreds)')\n","plt.title(\"Learning rate =\" + str(learning_rate))\n","plt.show()\n","### END CODE HERE ###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yI92fh4JXC1k"},"outputs":[],"source":["pred_train = predict(x_train, y_train, model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ehjcfSU2XD3-"},"outputs":[],"source":["#You can check for your validation accuracy here. (Optional)\n","### START CODE HERE ###\n","None\n","### END CODE HERE ###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YHFDuq2BQ2qI"},"outputs":[],"source":["pred_test = predict(X_test, None, model)\n","outputs[\"advanced_pred_test\"] = pred_test\n","outputs[\"advanced_layers_dims\"] = layers_dims\n","outputs[\"advanced_activation_fn\"] = activation_fn\n","outputs[\"advanced_loss_function\"] = loss_function\n","outputs[\"advanced_alpha\"] = alpha\n","outputs[\"advanced_gamma\"] = gamma\n","advanced_model_parameters = []\n","for advanced_linear in model.linear:\n","    advanced_model_parameters.append(advanced_linear.parameters)\n","outputs[\"advanced_model_parameters\"] = advanced_model_parameters"]},{"cell_type":"markdown","metadata":{"id":"WXGnS3HQeNUc"},"source":["# Submit prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"twMsmXbQeDL_"},"outputs":[],"source":["# sanity check\n","assert list(outputs.keys()) == ['linear_forward',\\\n","    'linear_backward',\\\n","    'linear_update_parameters',\\\n","    'sigmoid',\\\n","    'relu',\\\n","    'softmax',\\\n","    'sigmoid_backward',\\\n","    'relu_backward',\\\n","    'softmax_CCE_backward',\\\n","    'softmax_Focal_backward',\\\n","    'model_forward_sigmoid',\\\n","    'model_forward_relu',\\\n","    'model_forward_softmax',\\\n","    'model_backward_sigmoid',\\\n","    'model_backward_relu',\\\n","    'model_update_parameters',\\\n","    'compute_BCE_loss',\\\n","    'compute_CCE_loss',\\\n","    'compute_focal_loss',\\\n","    'basic_pred_test',\\\n","    'basic_layers_dims',\\\n","    'basic_activation_fn',\\\n","    'basic_loss_function',\\\n","    'basic_alpha',\\\n","    'basic_gamma',\\\n","    'basic_model_parameters',\\\n","    'advanced_pred_test',\\\n","    'advanced_layers_dims',\\\n","    'advanced_activation_fn',\\\n","    'advanced_loss_function',\\\n","    'advanced_alpha',\\\n","    'advanced_gamma',\\\n","    'advanced_model_parameters'],\\\n","\"You're missing something, please restart the kernel and run the code from begining to the end. If the same error occurs, maybe you deleted some outputs, check the template to find the missing parts!\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bCJ0XTO_zE8A"},"outputs":[],"source":["np.save(\"output.npy\", outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2XTk5AdzyHkf"},"outputs":[],"source":["# sanity check\n","submit = np.load(\"output.npy\", allow_pickle=True).item()\n","for key, value in submit.items():\n","  print(str(key) + \"： \" + str(type(value)))"]},{"cell_type":"markdown","metadata":{"id":"HXwjr9APIKt2"},"source":["**Notice**: We will not check the data type after \"compute_focal_loss\", (since you might be using focal loss in basic, then your basic_alpha wouldn't be None), so it is fine if you have different data type after \"compute_focal_loss\"."]},{"cell_type":"markdown","metadata":{"id":"trQqZni7jhP0"},"source":["Expected output: <br>\n","<small>\n","linear_forward： <class 'tuple'> <br>\n","linear_backward： <class 'tuple'> <br>\n","linear_update_parameters： <class 'dict'> <br>\n","sigmoid： <class 'tuple'> <br>\n","relu： <class 'tuple'> <br>\n","softmax： <class 'tuple'> <br>\n","sigmoid_backward： <class 'numpy.ndarray'> <br>\n","relu_backward： <class 'numpy.ndarray'> <br>\n","softmax_CCE_backward： <class 'numpy.ndarray'> <br>\n","softmax_Focal_backward： <class 'numpy.ndarray'> <br>\n","model_forward_sigmoid： <class 'tuple'> <br>\n","model_forward_relu： <class 'tuple'> <br>\n","model_forward_softmax： <class 'tuple'> <br>\n","model_backward_sigmoid： <class 'tuple'> <br>\n","model_backward_relu： <class 'tuple'> <br>\n","model_update_parameters： <class 'dict'> <br>\n","compute_BCE_loss： <class 'numpy.float64'> <br>\n","compute_CCE_loss： <class 'numpy.float64'> <br>\n","compute_focal_loss： <class 'numpy.float64'> <br>\n","basic_pred_test： <class 'numpy.ndarray'> <br>\n","basic_layers_dims： <class 'list'> <br>\n","basic_activation_fn： <class 'list'> <br>\n","basic_loss_function： <class 'str'> <br>\n","basic_alpha： <class 'NoneType'> <br>\n","basic_gamma： <class 'NoneType'> <br>\n","basic_model_parameters： <class 'list'> <br>\n","advanced_pred_test： <class 'numpy.ndarray'> <br>\n","advanced_layers_dims： <class 'list'> <br>\n","advanced_activation_fn： <class 'list'> <br>\n","advanced_loss_function： <class 'str'> <br>\n","advanced_alpha： <class 'numpy.ndarray'> <br>\n","advanced_gamma： <class 'float'> <br>\n","advanced_model_parameters： <class 'list'> <br>\n","</small>"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"vscode":{"interpreter":{"hash":"8a5edab282632443219e051e4ade2d1d5bbc671c781051bf1437897cbdfea0f1"}}},"nbformat":4,"nbformat_minor":0}

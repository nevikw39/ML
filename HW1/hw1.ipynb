{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Te27fi-0pP"
      },
      "source": [
        "# **HW1: Regression**\n",
        "In *assignment 1*, you need to finish:\n",
        "\n",
        "1.  Basic Part: Implement two regression models to predict the Systolic blood pressure (SBP) of a patient. You will need to implement **both Matrix Inversion and Gradient Descent**.\n",
        "\n",
        "\n",
        "> *   Step 1: Split Data\n",
        "> *   Step 2: Preprocess Data\n",
        "> *   Step 3: Implement Regression\n",
        "> *   Step 4: Make Prediction\n",
        "> *   Step 5: Train Model and Generate Result\n",
        "\n",
        "2.  Advanced Part: Implement one regression model to predict the SBP of multiple patients in a different way than the basic part. You can choose **either** of the two methods for this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wDdnos-4uUv"
      },
      "source": [
        "# **1. Basic Part (55%)**\n",
        "In the first part, you need to implement the regression to predict SBP from the given DBP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_EVqWlB-DTF"
      },
      "source": [
        "## 1.1 Matrix Inversion Method (25%)\n",
        "\n",
        "\n",
        "*   Save the prediction result in a csv file **hw1_basic_mi.csv**\n",
        "*   Print your coefficient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzCR7vk9BFkf"
      },
      "source": [
        "### *Import Packages*\n",
        "\n",
        "> Note: You **cannot** import any other package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HL5XjqFf4wSj",
        "tags": [
          "imports"
        ]
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnWjrzi0dMPz"
      },
      "source": [
        "### *Global attributes*\n",
        "Define the global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWLDPOlHBbcK"
      },
      "outputs": [],
      "source": [
        "training_dataroot = 'hw1_basic_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot = 'hw1_basic_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "output_dataroot = 'hw1_basic_mi.csv'         # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "training_datalist = pd.read_csv(training_dataroot) # Training datalist, saved as numpy array\n",
        "testing_datalist = pd.read_csv(testing_dataroot)   # Testing datalist, saved as numpy array\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsFC-cvqIcYK"
      },
      "source": [
        "You can add your own global attributes here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUbS2BEgcut6"
      },
      "outputs": [],
      "source": [
        "deg = 1 + 1\n",
        "n_train = n_valid = n_test = 0\n",
        "x_train = y_train = x_valid = y_valid = x_test = y_test = w = np.array([])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": [
          "MAPE"
        ]
      },
      "outputs": [],
      "source": [
        "def MAPE(y_hat: np._typing.NDArray, y: np._typing.NDArray) -> float:\n",
        "    return np.mean(np.abs((y_hat - y) / y)) * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUoRFoQjBW5S"
      },
      "source": [
        "### *Load the Input File*\n",
        "First, load the basic input file **hw1_basic_training.csv** and **hw1_basic_testing.csv**\n",
        "\n",
        "Input data would be stored in *training_datalist* and *testing_datalist*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dekR1KnqBtI6"
      },
      "outputs": [],
      "source": [
        "# Read input csv to datalist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kYPuikLCFx4"
      },
      "source": [
        "### *Implement the Regression Model*\n",
        "\n",
        "> Note: It is recommended to use the functions we defined, you can also define your own functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWwdx06JNEYs"
      },
      "source": [
        "#### Step 1: Split Data\n",
        "Split data in *training_datalist* into training dataset and validation dataset\n",
        "* Validation dataset is used to validate your own model without the testing data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USDciENcB-5F"
      },
      "outputs": [],
      "source": [
        "def SplitData():\n",
        "    global n_train, n_valid, n_test, x_train, x_valid, x_test, y_train, y_valid\n",
        "    np.random.shuffle(training_datalist.values)\n",
        "    n = len(training_datalist)\n",
        "    n_train = n - n // 16\n",
        "    n_valid = n // 16\n",
        "    train_dbp = training_datalist[\"dbp\"].to_numpy()\n",
        "    train_sbp = training_datalist[\"sbp\"].to_numpy()\n",
        "    x_train, x_valid = train_dbp[:n_train], train_dbp[n_train:]\n",
        "    y_train, y_valid = train_sbp[:n_train], train_sbp[n_train:]\n",
        "    n_test = len(testing_datalist)\n",
        "    x_test = testing_datalist[\"dbp\"].to_numpy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-3Qln4aNgVy"
      },
      "source": [
        "#### Step 2: Preprocess Data\n",
        "Handle the unreasonable data\n",
        "> Hint: Outlier and missing data can be handled by removing the data or adding the values with the help of statistics  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXvW1n_5NkQ5"
      },
      "outputs": [],
      "source": [
        "def PreprocessData():\n",
        "    global x_train, y_train\n",
        "    mu_x = np.mean(x_train)\n",
        "    mu_y = np.mean(y_train)\n",
        "    sigma_x = np.std(x_train)\n",
        "    sigma_y = np.std(y_train)\n",
        "    cond = np.logical_and(np.abs(x_train - mu_x) <= 3 * sigma_x, np.abs(y_train - mu_y) <= 3 * sigma_y)\n",
        "    x_train, y_train = x_train[cond], y_train[cond]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDLpJmQUN3V6"
      },
      "source": [
        "#### Step 3: Implement Regression\n",
        "> use Matrix Inversion to finish this part\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tx9n1_23N8C0"
      },
      "outputs": [],
      "source": [
        "def MatrixInversion():\n",
        "    global w, deg\n",
        "    w = np.linalg.pinv(np.vander(x_train, deg)) @ y_train\n",
        "    # w = np.linalg.lstsq(np.vander(x_train, deg), y_train, None)[0]\n",
        "    y_valid_hat = np.vander(x_valid, deg) @ w\n",
        "    print(f\"Validation loss: {MAPE(y_valid_hat, y_valid)}\")\n",
        "    plt.scatter(x_valid, y_valid)\n",
        "    plt.scatter(x_train, y_train)\n",
        "    plt.axline((0, w[-1]), slope=w[-2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NxRNFwyN8xd"
      },
      "source": [
        "#### Step 4: Make Prediction\n",
        "Make prediction of testing dataset and store the value in *output_datalist*\n",
        "The final *output_datalist* should look something like this \n",
        "> [ [100], [80], ... , [90] ] where each row contains the predicted SBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKlDIC2-N_lk"
      },
      "outputs": [],
      "source": [
        "def MakePrediction():\n",
        "    global y_test\n",
        "    y_test = np.vander(x_test, deg) @ w\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCd0Z6izOCwq"
      },
      "source": [
        "#### Step 5: Train Model and Generate Result\n",
        "\n",
        "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
        "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be:\n",
        "```\n",
        "3 2 1\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCL92EPKOFIn"
      },
      "outputs": [],
      "source": [
        "SplitData()\n",
        "PreprocessData()\n",
        "MatrixInversion()\n",
        "MakePrediction()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(' '.join(str(i) for i in w))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8Jhd8wAOk3D"
      },
      "source": [
        "### *Write the Output File*\n",
        "Write the prediction to output csv\n",
        "> Format: 'sbp'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYQVYLlKOtDB"
      },
      "outputs": [],
      "source": [
        "np.savetxt(output_dataroot, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J3WOhglA9ML"
      },
      "source": [
        "## 1.2 Gradient Descent Method (30%)\n",
        "\n",
        "\n",
        "*   Save the prediction result in a csv file **hw1_basic_gd.csv**\n",
        "*   Output your coefficient update in a csv file **hw1_basic_coefficient.csv**\n",
        "*   Print your coefficient\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkMqa_xjXhEv"
      },
      "source": [
        "### *Global attributes*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNZtRWUeXpEu"
      },
      "outputs": [],
      "source": [
        "output_dataroot = 'hw1_basic_gd.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "coefficient_output_dataroot = 'hw1_basic_coefficient.csv'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5DeHxdLdai3"
      },
      "source": [
        "Your own global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2IO5tYSdaFd"
      },
      "outputs": [],
      "source": [
        "valid_loss_history = []\n",
        "w_history = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVBLT1aqXuW0"
      },
      "source": [
        "### *Implement the Regression Model*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecPWpcOnXhCZ"
      },
      "source": [
        "#### Step 1: Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PEf_qGvYHu0"
      },
      "outputs": [],
      "source": [
        "# def SplitData():"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpSoPDPKX56w"
      },
      "source": [
        "#### Step 2: Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLTXOWRwYHiS"
      },
      "outputs": [],
      "source": [
        "# def PreprocessData():"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV_y82gXX6a-"
      },
      "source": [
        "#### Step 3: Implement Regression\n",
        "> use Gradient Descent to finish this part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-635Ee00YHTE"
      },
      "outputs": [],
      "source": [
        "def GradientDescent():\n",
        "    global w\n",
        "    w = np.array([1., 50.])\n",
        "    n = len(x_train)\n",
        "    X_train = np.vander(x_train, deg)\n",
        "    X_valid = np.vander(x_valid, deg)\n",
        "    alpha = 1e-4 # learning rate\n",
        "    for i in range(128):\n",
        "        gradient = -2 / n * ((y_train - (X_train @ w)) @ X_train)\n",
        "        w -= alpha * math.exp(-0.1 * i) * gradient\n",
        "        w_history.append(np.copy(w))\n",
        "        valid_loss = MAPE(X_valid @ w, y_valid)\n",
        "        valid_loss_history.append(valid_loss)\n",
        "        print(f\"Epoch {i}: validation loss = {valid_loss}\")\n",
        "        if len(valid_loss_history) > 5 and valid_loss_history[-1] > min(valid_loss_history[-6:-1]):\n",
        "            break\n",
        "    if valid_loss_history[-1] > 10:\n",
        "            raise Exception(\"Not converge in 128 epochs\")\n",
        "    plt.scatter(x_valid, y_valid)\n",
        "    plt.scatter(x_train, y_train)\n",
        "    plt.axline((0, w[-1]), slope=w[-2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLuPxs2ZX21S"
      },
      "source": [
        "#### Step 4: Make Prediction\n",
        "\n",
        "Make prediction of testing dataset and store the values in *output_datalist*\n",
        "The final *output_datalist* should look something like this \n",
        "> [ [100], [80], ... , [90] ] where each row contains the predicted SBP\n",
        "\n",
        "Remember to also store your coefficient update in *coefficient_output*\n",
        "The final *coefficient_output* should look something like this\n",
        "> [ [1, 0, 3, 5], ... , [0.1, 0.3, 0.2, 0.5] ] where each row contains the [w0, w1, ..., wn] of your coefficient\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pnNDlQeYGtE"
      },
      "outputs": [],
      "source": [
        "# def MakePrediction():"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IScbxxMAYAgZ"
      },
      "source": [
        "#### Step 5: Train Model and Generate Result\n",
        "\n",
        "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
        "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be:\n",
        "```\n",
        "3 2 1\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90EisOc7YG-N"
      },
      "outputs": [],
      "source": [
        "GradientDescent()\n",
        "MakePrediction()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(' '.join(str(i) for i in w))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1DpV_HcYFpl"
      },
      "source": [
        "### *Write the Output File*\n",
        "\n",
        "Write the prediction to output csv\n",
        "> Format: 'sbp'\n",
        "\n",
        "**Write the coefficient update to csv**\n",
        "> Format: 'w0', 'w1', ..., 'wn'\n",
        ">*   The number of columns is based on your number of coefficient\n",
        ">*   The number of row is based on your number of iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLSHgpDvDXNI"
      },
      "outputs": [],
      "source": [
        "np.savetxt(output_dataroot, y_test)\n",
        "np.savetxt(coefficient_output_dataroot, w_history, delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx4408qg4xMQ"
      },
      "source": [
        "# **2. Advanced Part (40%)**\n",
        "In the second part, you need to implement the regression in a different way than the basic part to help your predictions of multiple patients SBP.\n",
        "\n",
        "You can choose **either** Matrix Inversion or Gradient Descent method.\n",
        "\n",
        "The training data will be in **hw1_advanced_training.csv** and the testing data will be in **hw1_advanced_testing.csv**.\n",
        "\n",
        "Output your prediction in **hw1_advanced.csv**\n",
        "\n",
        "Notice:\n",
        "> You cannot import any other package other than those given\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Input the training and testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "v66HUClZcxaE"
      },
      "outputs": [],
      "source": [
        "training_dataroot = 'hw1_advanced_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot = 'hw1_advanced_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "output_dataroot = 'hw1_advanced.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "training_datalist = pd.read_csv(training_dataroot) # Training datalist, saved as numpy array\n",
        "testing_datalist = pd.read_csv(testing_dataroot) # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 220 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 10.554067491166713\n",
            "Validation loss: 9.646004842597025\n",
            "Validation loss: 13.573581480732697\n",
            "Validation loss: 19.055969568728095\n",
            "Validation loss: 8.819969310599102\n",
            "Validation loss: 10.885470216796218\n",
            "Validation loss: 9.87486742721214\n",
            "Validation loss: 10.829962335976635\n",
            "Validation loss: 15.63890473815073\n",
            "Validation loss: 7.812019369547292\n",
            "Validation loss: 10.200942935384653\n"
          ]
        }
      ],
      "source": [
        "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    return (\n",
        "        df.drop(columns=[\"subject_id\", \"charttime\"])\n",
        "          .pipe(lambda x: x[(x - x.mean()).abs() / x.std() <= 3])\n",
        "          .ffill().bfill()\n",
        "    )\n",
        "\n",
        "\n",
        "def split(df: pd.DataFrame) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    n = len(df) // 16\n",
        "    y = df[\"sbp\"].to_numpy()\n",
        "    X = df.assign(sbp=1).to_numpy()\n",
        "    return X[:-n], X[-n:], y[:-n], y[-n:]\n",
        "\n",
        "\n",
        "w = {}\n",
        "grp = training_datalist.groupby(\"subject_id\")\n",
        "for g in grp.groups:\n",
        "    X_train, X_valid, y_train, y_valid = split(preprocess(grp.get_group(g)).sample(frac=1))\n",
        "    w[g], *_ = np.linalg.lstsq(X_train, y_train, rcond=None)\n",
        "    print(f\"Validation loss: {MAPE(X_valid @ w[g], y_valid)}\")\n",
        "\n",
        "\n",
        "grp = testing_datalist.groupby(\"subject_id\")\n",
        "for g in grp.groups:\n",
        "    output_datalist.extend(preprocess(grp.get_group(g)).assign(sbp=1).to_numpy() @ w[g])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Output your Prediction\n",
        "\n",
        "> your filename should be **hw1_advanced.csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.savetxt(output_dataroot, output_datalist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtgCJU7FPeJL"
      },
      "source": [
        "# Report *(5%)*\n",
        "\n",
        "Report should be submitted as a pdf file **hw1_report.pdf**\n",
        "\n",
        "*   Briefly describe the difficulty you encountered\n",
        "*   Summarize your work and your reflections\n",
        "*   No more than one page\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlEE53_MPf4W"
      },
      "source": [
        "# Save the Code File\n",
        "Please save your code and submit it as an ipynb file! (**hw1.ipynb**)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
